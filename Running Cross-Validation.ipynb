{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Running cross-validation to estimate performance of system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "load_loc = '/home/sfvn/Dropbox/DeepFactData/annotated/' #SPECIFY PATH TO DATA\n",
    "with open(load_loc+\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start time', 'end time', 'program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2346,    0],\n",
       "       [  76,  123]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['data'][:,4] # sentences\n",
    "y = data['data'][:,6] # claim indices\n",
    "N = len(X)\n",
    "\n",
    "features = data['features']\n",
    "print(features)\n",
    "\n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(N)])\n",
    "\n",
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfvn/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n48UA7msGWUQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d046647f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get number of programs\n",
    "program_ids = data['data'][:,2]\n",
    "unique_programs = np.unique(program_ids)\n",
    "NUM_PROGRAMS = len(unique_programs)                   \n",
    "loo = LeaveOneOut()\n",
    "\n",
    "class_performance_precision = []\n",
    "class_performance_recall = []\n",
    "\n",
    "for train,test in loo.split(unique_programs):\n",
    "    # Extract training and test data\n",
    "    train_idx = program_ids!=unique_programs[test]\n",
    "    test_idx = program_ids==unique_programs[test]\n",
    "    X_test = X_bow[test_idx]\n",
    "    X_train = X_bow[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    \n",
    "    # Train model\n",
    "    logistic = linear_model.LogisticRegression()\n",
    "    logistic.fit(X=X_train,y=y_train)\n",
    "    ypred = logistic.predict(X_test) \n",
    "\n",
    "    # Evaluate\n",
    "    class_performance_precision += [precision_score(y_test,ypred)]\n",
    "    class_performance_recall += [recall_score(y_test,ypred)]\n",
    "\n",
    "plt.boxplot( [class_performance_precision, class_performance_recall] )\n",
    "plt.xlabel( ['Precision', 'Recall'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['program1', 'program2', 'program3', 'program4', 'program5',\n",
       "       'program6', 'program7'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "\n",
    "def leave_one_program_out_cv(data, model_list, eval_functions = [accuracy_score]):\n",
    "    # TODO: Change model_list to class_list and initialize objects in model-loop\n",
    "    # Get program ids and number of programs\n",
    "    program_ids = data['data'][:,2]\n",
    "    unique_programs = np.unique(program_ids)\n",
    "    NUM_PROGRAMS = len(unique_programs)                   \n",
    "    loo = LeaveOneOut()\n",
    "    classification_results = np.empty( (NUM_PROGRAMS, len(model_list), len(eval_functions)))\n",
    "    classification_results.fill(np.nan)\n",
    "    \n",
    "    # Loop over programs\n",
    "    p = 0\n",
    "    for train,test in loo.split(unique_programs):\n",
    "        train_idx = program_ids!=unique_programs[test]\n",
    "        test_idx = program_ids==unique_programs[test]\n",
    "        \n",
    "        print('Welcome to program %i' %(p+1) )\n",
    "        print('Number of training examples %i'%(np.sum(train_idx)) )\n",
    "        print('Number of test examples %i'%(np.sum(test_idx)) )\n",
    "\n",
    "        training_data, test_data = data_to_tensors(data, train_idx, test_idx)\n",
    "        m = 0\n",
    "        for model in model_list:\n",
    "            # Initalizize TF.seesion... and clear previous?\n",
    "            tfsess = tf.Session()\n",
    "            model.fit(training_data, tfsess)\n",
    "            y_pred = model.predict(test_data, tfsess)\n",
    "            # Evaluate with eval_functions\n",
    "            e=0\n",
    "            for evalf in eval_functions:\n",
    "                classification_results[p,m,e] = evalf(test_data['labels'], y_pred)\n",
    "                e += 1\n",
    "                \n",
    "            m+=1\n",
    "        \n",
    "        print(\"Done with training and evaluation! ---\")\n",
    "        p+=1\n",
    "    return classification_results\n",
    "    \n",
    "def data_to_tensors(data, train_indices=None, test_indices=None):\n",
    "    # TODO: embedding input (string 'bow', 'word2vec', 'glove', ... )\n",
    "    # Performs neccesary feature extraction and test/training split\n",
    "    # Returns data transformed in multiple ways tensors: \n",
    "    #    char: Char-based \n",
    "    #    pos: Part-of-Speech tagging\n",
    "    #    word2vec: Word2vec (or someother subspace..)\n",
    "    #    bow: Bag-Of-Words\n",
    "    # Furthermore returns for each sample the binary vector y (labels)\n",
    "    \n",
    "    data_train = dict()\n",
    "    data_test = dict()\n",
    "    \n",
    "    # Extract relevant data from table\n",
    "    X = data['data'][:,4] # sentences\n",
    "    y = data['data'][:,6] # claim indices\n",
    "    N = len(X)\n",
    "\n",
    "    # If no test/train split is specified return everything in training\n",
    "    if train_indices is None and test_indices is None:\n",
    "        train_indices=np.ones(N, dtype=bool)\n",
    "        \n",
    "\n",
    "    # Pr. sample label-vector    \n",
    "    y = np.asarray([y[i] is not None for i in range(N)])    \n",
    "    data_train['labels'] = y[train_indices]\n",
    "    if train_indices is not None and test_indices is not None:\n",
    "        data_test['labels'] = y[test_indices]\n",
    "\n",
    "    \n",
    "    # Char\n",
    "    # ...\n",
    "    \n",
    "    # Pos\n",
    "    # ...\n",
    "    \n",
    "    # Word2Vec\n",
    "    # ...\n",
    "    \n",
    "    # Bag-Of-Words\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(X)\n",
    "    data_train['bow'] = X_bow[train_indices,:]\n",
    "    if train_indices is not None and test_indices is not None:\n",
    "        data_test['bow']= X_bow[test_indices,:]\n",
    "    \n",
    "    \n",
    "    return data_train, data_test\n",
    "\n",
    "\n",
    "#class TensorFlowModel:\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mymodels.baselines import MyLogisticRegression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to program 1\n",
      "Number of training examples 2029\n",
      "Number of test examples 516\n",
      "Epoch: 0001 cost= 0.052224949\n",
      "Epoch: 0002 cost= 0.052222583\n",
      "Epoch: 0003 cost= 0.052221563\n",
      "Epoch: 0004 cost= 0.052220561\n",
      "Epoch: 0005 cost= 0.052218858\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 2\n",
      "Number of training examples 2225\n",
      "Number of test examples 320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfvn/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.051657040\n",
      "Epoch: 0002 cost= 0.051655069\n",
      "Epoch: 0003 cost= 0.051654309\n",
      "Epoch: 0004 cost= 0.051653452\n",
      "Epoch: 0005 cost= 0.051651634\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 3\n",
      "Number of training examples 2103\n",
      "Number of test examples 442\n",
      "Epoch: 0001 cost= 0.059261095\n",
      "Epoch: 0002 cost= 0.059258666\n",
      "Epoch: 0003 cost= 0.059257809\n",
      "Epoch: 0004 cost= 0.059256326\n",
      "Epoch: 0005 cost= 0.059253622\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 4\n",
      "Number of training examples 2228\n",
      "Number of test examples 317\n",
      "Epoch: 0001 cost= 0.052851539\n",
      "Epoch: 0002 cost= 0.052849550\n",
      "Epoch: 0003 cost= 0.052848786\n",
      "Epoch: 0004 cost= 0.052847899\n",
      "Epoch: 0005 cost= 0.052846048\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 5\n",
      "Number of training examples 2236\n",
      "Number of test examples 309\n",
      "Epoch: 0001 cost= 0.049242124\n",
      "Epoch: 0002 cost= 0.049240317\n",
      "Epoch: 0003 cost= 0.049239483\n",
      "Epoch: 0004 cost= 0.049238656\n",
      "Epoch: 0005 cost= 0.049237199\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 6\n",
      "Number of training examples 2211\n",
      "Number of test examples 334\n",
      "Epoch: 0001 cost= 0.056063555\n",
      "Epoch: 0002 cost= 0.056061447\n",
      "Epoch: 0003 cost= 0.056060649\n",
      "Epoch: 0004 cost= 0.056059536\n",
      "Epoch: 0005 cost= 0.056057300\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n",
      "Welcome to program 7\n",
      "Number of training examples 2238\n",
      "Number of test examples 307\n",
      "Epoch: 0001 cost= 0.057890374\n",
      "Epoch: 0002 cost= 0.057888266\n",
      "Epoch: 0003 cost= 0.057887550\n",
      "Epoch: 0004 cost= 0.057886612\n",
      "Epoch: 0005 cost= 0.057884675\n",
      "Optimization Finished!\n",
      "Done with training and evaluation! ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.91085271,  0.        ]],\n",
       "\n",
       "       [[ 0.896875  ,  0.        ]],\n",
       "\n",
       "       [[ 0.95701357,  0.        ]],\n",
       "\n",
       "       [[ 0.90851735,  0.        ]],\n",
       "\n",
       "       [[ 0.87055016,  0.        ]],\n",
       "\n",
       "       [[ 0.94011976,  0.        ]],\n",
       "\n",
       "       [[ 0.96091205,  0.        ]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "leave_one_program_out_cv(data, [MyLogisticRegression()], eval_functions=[accuracy_score, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tensors = data_to_tensors(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5e635c6d7452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
