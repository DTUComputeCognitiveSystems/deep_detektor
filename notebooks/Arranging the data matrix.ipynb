{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import csv\n",
    "from pathlib import Path\n",
    "sys.path.append(\"..\")\n",
    "from project_paths import ProjectPaths\n",
    "ProjectPaths.set_path_to_repository(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available project.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir                                : ..\\..\\data\n",
      "deep_fact_dir                           : ..\\..\\data\\DeepFactData\n",
      "annotated                               : ..\\..\\data\\DeepFactData\\annotated\n",
      "data_matrix_path                        : ..\\..\\data\\DeepFactData\\annotated\\data_matrix_sample_programs.csv\n",
      "nlp_data_dir                            : ..\\..\\data\\DeepFactData\\nlp_data\n",
      "embeddings_file                         : ..\\..\\data\\DeepFactData\\nlp_data\\embeddings.csv\n",
      "pos_tags_file                           : ..\\..\\data\\DeepFactData\\nlp_data\\pos_tags.csv\n",
      "speller_dir                             : ..\\..\\data\\DeepFactData\\spelling_model\n",
      "speller_char_vocab_file                 : ..\\..\\data\\DeepFactData\\spelling_model\\char_embedding.json\n",
      "speller_encoder_checkpoint_file         : ..\\..\\data\\DeepFactData\\spelling_model\\checkpoint\\speller_encode.ckpt\n",
      "speller_results_file                    : ..\\..\\data\\DeepFactData\\spelling_model\\results.json\n",
      "speller_translator_file                 : ..\\..\\data\\DeepFactData\\spelling_model\\string_translator.json\n",
      "dr_detektor_automatic_fact_checking_dir : ..\\..\\data\\DRDetektorAutomaticFactChecking\n",
      "annotated_subtitles                     : ..\\..\\data\\DRDetektorAutomaticFactChecking\\annotatorProgram\\annotatedSubtitles\n",
      "results                                 : ..\\..\\data\\results\n"
     ]
    }
   ],
   "source": [
    "ProjectPaths.print_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_loc = ProjectPaths.annotated\n",
    "loc_ann_data = ProjectPaths.annotated_subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DebattenAnnotatedDatacleaner:\n",
    "    \"\"\"\n",
    "    Takes the annotated programs, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialises class and input, output locations\n",
    "    def __init__(self, loc_ann=None, loc_out=None):\n",
    "        self.loc_ann_subtitles = loc_ann\n",
    "        self.loc_out_subtitles = loc_out\n",
    "    \n",
    "    def setAnnotatedFilesLocation(self, new_loc):\n",
    "        self.loc_ann_subtitles = new_loc\n",
    "        \n",
    "    def setOutputFilesLocation(self, new_loc):\n",
    "        self.loc_out_subtitles = new_loc\n",
    "    \n",
    "    def getFileLocation(self, disp=True):\n",
    "        \n",
    "        if disp:\n",
    "            if self.loc_ann_subtitles is None:\n",
    "                print('Annotated subtitles are not specified!')\n",
    "            else:\n",
    "                print('Annotated subtitles are loaded from \"{:s}\"'.format(self.loc_ann_subtitles))\n",
    "\n",
    "            if self.loc_out_subtitles is None:\n",
    "                print('Save location is not specified!')\n",
    "            else:\n",
    "                print('Save location is \"{:s}\"'.format(self.loc_out_subtitles))\n",
    "       \n",
    "        return self.loc_ann_subtitles, self.loc_out_subtitles\n",
    "    \n",
    "    def getFilePaths(self):\n",
    "        files = self.loc_ann_subtitles.glob(\"*.txt\")\n",
    "        return list(files)\n",
    "    \n",
    "    def getProgramAndSentences(self, f_path):\n",
    "        \"\"\"Gets the program id, sentences id and sentences from a document\"\"\"\n",
    "        with f_path.open('r', encoding=\"utf-8\") as f:\n",
    "            doc = f.read()\n",
    "\n",
    "        #Find program id\n",
    "        m_program_id = re.compile('program[\\d]+')\n",
    "        m = re.search(m_program_id, doc)\n",
    "        program_id = m.group()\n",
    "\n",
    "        sentences = doc.split('<p ')\n",
    "        m_sentence_id = re.compile('id=\"[\\d]+\">')\n",
    "\n",
    "        # Finds the sentence ids and removes html stuff from the begining of each sentence\n",
    "        sentences_id = []\n",
    "        for i in range(len(sentences)):\n",
    "            match = re.search(m_sentence_id, sentences[i])\n",
    "            if not match:\n",
    "                sentences[i] = None\n",
    "            else:\n",
    "                sentences_id.append(int(match.group()[4:-2]))\n",
    "\n",
    "                start_from = sentences[i].find('>')+1\n",
    "                sentences[i] = sentences[i][start_from:]\n",
    "\n",
    "        sentences = list(filter(None, sentences)) # Remove None elements\n",
    "        assert(len(sentences)==len(sentences_id))\n",
    "\n",
    "        return program_id, sentences_id, sentences\n",
    "    \n",
    "    # Finds highligted text including its surrounding patttern\n",
    "    def findHighlights(self,s):\n",
    "        m_highlight = re.compile('<span id=\"highlight[\"\\w\\d ]+class=\"highlight[\\w\"]+>[\\w\\d. ,!?%]+</span>')\n",
    "        return re.findall(m_highlight, s)\n",
    "    \n",
    "    # Extracts highlighted text only\n",
    "    def extractHighlights(self, s_matches):#Extracted the text highlighted\n",
    "        m_high_text = re.compile('\">[\\w\\d ,.!?%]+</')\n",
    "        high_text = [re.findall(m_high_text, s_matches[i])[0][2:-2] for i in range(len(s_matches))]\n",
    "        return [s.lstrip().rstrip() for s in high_text]\n",
    "    \n",
    "    # Removes html tags (and crap) from the string.\n",
    "    def cleanSentence(self, s, disp=False):\n",
    "\n",
    "        m_crap = re.compile('<[\\w\\d \"=/]+>')\n",
    "        s_crap_free = s\n",
    "        for pattern in re.findall(m_crap, s): \n",
    "            if disp: print(pattern)\n",
    "            s_crap_free = s_crap_free.replace(pattern,'')\n",
    "\n",
    "        #s_crap_free = re.sub('id=\"[\\d]+\">','',s_crap_free) # only during dev\n",
    "\n",
    "        s_crap_free = s_crap_free.replace('\\t',' ') # removes tabs\n",
    "        s_crap_free = re.sub(' +',' ', s_crap_free) # removes excess spaces\n",
    "        return s_crap_free.lstrip().rstrip()\n",
    "\n",
    "    def getHighlight_indices(self,s,s_highlighted):\n",
    "        \n",
    "        # Two heuristic for correcting partially highlighted words.\n",
    "        def getLeadingSpace(s,start_idx):\n",
    "            # Finds first leading space before index \"start_idx\" in s\n",
    "            if start_idx < 0:\n",
    "                return 0\n",
    "            elif s[start_idx] is ' ' :\n",
    "                return start_idx+1\n",
    "            else:\n",
    "                return getLeadingSpace(s,start_idx-1)\n",
    "\n",
    "        def getTailingSpace(s,end_idx):\n",
    "            # Finds first trailing space after index \"end_idx\" in s\n",
    "            if end_idx >= len(s):\n",
    "                return len(s)\n",
    "            elif s[end_idx] is ' ' or end_idx == len(s):\n",
    "                return end_idx\n",
    "            else:\n",
    "                return getTailingSpace(s,end_idx+1)\n",
    "        \n",
    "        # Find the indicies of highlighted words\n",
    "        indices = []\n",
    "        # Get matched indices\n",
    "        for m in s_highlighted:\n",
    "            m_pattern = re.compile(m)\n",
    "            match = re.search(m_pattern, s)\n",
    "            if match:\n",
    "                indices.append([getLeadingSpace(s, match.start()), \n",
    "                                getTailingSpace(s, match.end())])\n",
    "            else:\n",
    "                print(match)\n",
    "                print(m)\n",
    "                print(s_highlighted)\n",
    "                print(s+'\\n')\n",
    "                \n",
    "        #print('\\n\\n')\n",
    "        return indices\n",
    "    \n",
    "    def getCleanedProgramSentences(self, sentences): \n",
    "        sentences_processed = [None]*len(sentences)\n",
    "        sentences_highlight = [None]*len(sentences)\n",
    "        sentences_highlight_ind = [None]*len(sentences)\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            sen = sentences[i]\n",
    "            raw_highlights = self.findHighlights(sen)\n",
    "            text_highlights = self.extractHighlights(raw_highlights)\n",
    "            \n",
    "            #Crap free verion\n",
    "            sentences_processed[i] = self.cleanSentence(sen)\n",
    "            #print('cleaned: '+sentences_processed[i])\n",
    "            indices_highlights = self.getHighlight_indices(sentences_processed[i], \n",
    "                                                                         text_highlights)\n",
    "            sentences_highlight_ind[i] = indices_highlights\n",
    "            \n",
    "            for idx in indices_highlights:\n",
    "                if sentences_highlight[i]:\n",
    "                     sentences_highlight[i] = sentences_highlight[i]+ ' [new claim]: '\\\n",
    "                                              +sentences_processed[i][idx[0]:idx[1]]\n",
    "                else:\n",
    "                    sentences_highlight[i] = sentences_processed[i][idx[0]:idx[1]]\n",
    "            \n",
    "            \n",
    "        return sentences_processed, sentences_highlight, sentences_highlight_ind\n",
    "    \n",
    "    # EXPERIMENTAL!!! Processing multi-claim paragraphs\n",
    "    def processMultiClaim(self,s,idx):\n",
    "        merge_claims = []\n",
    "        for c in range(len(idx)-1):\n",
    "            if abs(idx[c][1]-idx[c+1][0]) == 1: #It is the same claim\n",
    "                merge_claims.append(True)\n",
    "            else:\n",
    "                merge_claims.append(False)\n",
    "\n",
    "        new_s = []\n",
    "        new_idx = []\n",
    "        for c in range(len(idx)-1):\n",
    "            if merge_claims[c]:\n",
    "                start_id = idx[c][0]\n",
    "                end_id = idx[c+1][1]\n",
    "                new_idx.append([start_id, end_id])\n",
    "                new_s.append(s[start_id:end_id])\n",
    "            else:\n",
    "                if c > 0:\n",
    "                    new_s.append(' [new claim]: ')\n",
    "\n",
    "                start_id = idx[c][0]\n",
    "                end_id = idx[c][1]\n",
    "                new_idx.append([start_id, end_id])\n",
    "                new_s.append(s[start_id:end_id])\n",
    "\n",
    "        if not merge_claims[-1]:\n",
    "\n",
    "            new_s.append(' [new claim]: ')\n",
    "\n",
    "            start_id = idx[-1][0]\n",
    "            end_id = idx[-1][1]\n",
    "            new_idx.append([start_id, end_id])\n",
    "            new_s.append(s[start_id:end_id])\n",
    "\n",
    "\n",
    "        new_s = ''.join(new_s)\n",
    "        return new_s, new_idx\n",
    "    \n",
    "    def getAllCleanedProgramSentences(self,disp=False):\n",
    "        file_paths = self.getFilePaths()\n",
    "\n",
    "        all_program_id = [None]*len(file_paths)\n",
    "        all_sentences = [None]*len(file_paths)\n",
    "        all_sentences_id = [None]*len(file_paths)\n",
    "        all_highlights = [None]*len(file_paths)\n",
    "        all_highlights_ind = [None]*len(file_paths)\n",
    "        \n",
    "        total_claims = 0;\n",
    "        total_sentences = 0;\n",
    "        \n",
    "        for f in range(len(file_paths)):\n",
    "            all_program_id[f], all_sentences_id[f], sentences = \\\n",
    "                        self.getProgramAndSentences(file_paths[f])\n",
    "            if disp: print('Program id {:s}'.format(all_program_id[f]))\n",
    "            \n",
    "            all_sentences[f], all_highlights[f], all_highlights_ind[f] = \\\n",
    "                        self.getCleanedProgramSentences(sentences)\n",
    "            \n",
    "            num_claims = len(list(filter(None,all_highlights[f])))\n",
    "            if disp: print('\\tThere were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(num_claims\\\n",
    "                    ,len(sentences), num_claims/float(len(sentences))*100))\n",
    "                \n",
    "            total_claims = total_claims+num_claims\n",
    "            total_sentences = total_sentences + len(sentences)\n",
    "            \n",
    "        if disp: print('\\nIn total there were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(total_claims\\\n",
    "                , total_sentences, total_claims/float(total_sentences)*100))\n",
    "        \n",
    "        # ...\n",
    "        labels = ['program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n",
    "        \n",
    "        data = [ [None]*len(labels) for i in range(total_sentences)]\n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "        for p in range(len(all_program_id)):\n",
    "            \n",
    "            for si in range(len(all_sentences[p])):\n",
    "                data[i][0] = all_program_id[p]\n",
    "                \n",
    "                data[i][1] = all_sentences_id[p][si]\n",
    "                data[i][2] = all_sentences[p][si]\n",
    "                \n",
    "                if len(all_highlights_ind[p][si]) == 1:\n",
    "                    data[i][3] = all_highlights_ind[p][si]\n",
    "                    data[i][4] = all_highlights[p][si]\n",
    "                    \n",
    "                elif all_highlights_ind[p][si]:\n",
    "                    \n",
    "                    print('HELP')\n",
    "                    print(all_program_id[p])\n",
    "                    #print(all_sentences[p][si])\n",
    "                    print(all_highlights_ind[p][si])\n",
    "                    print(all_highlights[p][si])\n",
    "                    new_s, new_idx = self.processMultiClaim(all_sentences[p][si],\\\n",
    "                                                      all_highlights_ind[p][si])\n",
    "                    \n",
    "                    print('Trying to handle this multi-claim, is the output correct?')\n",
    "                    print(new_idx)\n",
    "                    print(new_s)\n",
    "                    print()\n",
    "                    \n",
    "                    data[i][3] = new_idx\n",
    "                    data[i][4] = new_s\n",
    "                \n",
    "                i = i+1\n",
    "            \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotatedData = DebattenAnnotatedDatacleaner(loc_ann_data)\n",
    "file_paths = annotatedData.getFilePaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test of file-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id is program10\n",
      "['<span id=\"highlight48\" class=\"highlight\">Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.</span>']\n",
      "['Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.']\n",
      "<span id=\"highlight48\" class=\"highlight\">\n",
      "</span>\n",
      "</p>\n",
      "\n",
      "\n",
      "Det er dem, der støtter ham. Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.\n",
      "[[29, 117]]\n"
     ]
    }
   ],
   "source": [
    "program_id, sentences_id, sentences = annotatedData.getProgramAndSentences(file_paths[1])\n",
    "print('Id is '+program_id)\n",
    "\n",
    "s = sentences[111]\n",
    "\n",
    "match_high = annotatedData.findHighlights(s)\n",
    "print(match_high)\n",
    "\n",
    "match_text_high = annotatedData.extractHighlights(match_high)\n",
    "print(match_text_high)\n",
    "\n",
    "s_crap_free = annotatedData.cleanSentence(s, disp=True)\n",
    "print('\\n\\n' +s_crap_free)\n",
    "\n",
    "print(annotatedData.getHighlight_indices(s_crap_free, match_text_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program id program1\n",
      "\tThere were 46 claims out of 516 sentences (8.91%)\n",
      "Program id program10\n",
      "\tThere were 7 claims out of 231 sentences (3.03%)\n",
      "Program id program2\n",
      "\tThere were 33 claims out of 320 sentences (10.31%)\n",
      "Program id program3\n",
      "\tThere were 20 claims out of 442 sentences (4.52%)\n",
      "Program id program4\n",
      "\tThere were 30 claims out of 317 sentences (9.46%)\n",
      "Program id program5\n",
      "\tThere were 42 claims out of 309 sentences (13.59%)\n",
      "Program id program6\n",
      "\tThere were 22 claims out of 334 sentences (6.59%)\n",
      "Program id program7\n",
      "\tThere were 12 claims out of 307 sentences (3.91%)\n",
      "Program id program8\n",
      "\tThere were 32 claims out of 406 sentences (7.88%)\n",
      "Program id program9\n",
      "\tThere were 22 claims out of 307 sentences (7.17%)\n",
      "\n",
      "In total there were 266 claims out of 3489 sentences (7.62%)\n",
      "HELP\n",
      "program3\n",
      "[[0, 18], [19, 60]]\n",
      "Jeg kan frygte, at [new claim]: DR1 bliver en slags TV2 i middelmådighed.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 60]]\n",
      "Jeg kan frygte, at DR1 bliver en slags TV2 i middelmådighed.\n",
      "\n",
      "HELP\n",
      "program4\n",
      "[[0, 57], [58, 136]]\n",
      "Mange af problemerne er skabt af den tidligere præsident, [new claim]: altså Bush fordi han har skubbet uro ind i områder, der før var under kontrol.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 136]]\n",
      "Mange af problemerne er skabt af den tidligere præsident, altså Bush fordi han har skubbet uro ind i områder, der før var under kontrol.\n",
      "\n",
      "HELP\n",
      "program5\n",
      "[[0, 71], [72, 112]]\n",
      "Det Europæiske Energiagentur har gjort op at Danmark er et af de lande, [new claim]: der er længst væk fra at opfylde målene.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 112]]\n",
      "Det Europæiske Energiagentur har gjort op at Danmark er et af de lande, der er længst væk fra at opfylde målene.\n",
      "\n",
      "HELP\n",
      "program5\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "\n",
      "HELP\n",
      "program6\n",
      "[[0, 60], [61, 105]]\n",
      "Efter syv år med fremgang er virkeligheden vendt for Venstre [new claim]: som får historiske smæk i meningsmålingerne.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 105]]\n",
      "Efter syv år med fremgang er virkeligheden vendt for Venstre som får historiske smæk i meningsmålingerne.\n",
      "\n",
      "HELP\n",
      "program6\n",
      "[[0, 9], [10, 38]]\n",
      "Jeg tror, [new claim]: vi får en stigende ledighed.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 38]]\n",
      "Jeg tror, vi får en stigende ledighed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, labels = annotatedData.getAllCleanedProgramSentences(disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine timestamps from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['program4', 'program2', 'program6', 'program5', 'program3', 'program7', 'program1', 'program10', 'program8', 'program9'])\n",
      "\n",
      "dict_keys(['start time', 'sentences', 'end time'])\n"
     ]
    }
   ],
   "source": [
    "with Path(save_loc, 'sample_programs.pickle').open('rb') as f:\n",
    "    sample_dict = pickle.load(f)\n",
    "    \n",
    "print(sample_dict.keys())\n",
    "print()\n",
    "print(sample_dict['program1'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start time', 'end time', 'program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n"
     ]
    }
   ],
   "source": [
    "N = len(data) # Observations\n",
    "features = ['start time', 'end time']\n",
    "[features.append(lab) for lab in labels]\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the time from the processed data\n",
    "start_times = []\n",
    "end_times = []\n",
    "processed_programs = []\n",
    "\n",
    "for i in range(N):  \n",
    "    pro_id = data[i][0]\n",
    "    \n",
    "    # Note sample_dict[pro_id]['start time'] is a list of start times for all sentences\n",
    "    if pro_id not in processed_programs:\n",
    "        [start_times.append(t) for t in sample_dict[pro_id]['start time']]\n",
    "        [end_times.append(t) for t in sample_dict[pro_id]['end time']]\n",
    "        \n",
    "        processed_programs.append(pro_id)\n",
    "# Sanity check        \n",
    "assert(len(start_times) == len(end_times))\n",
    "assert(len(start_times) == N)\n",
    "\n",
    "# Concat data\n",
    "X = np.concatenate((np.asarray([start_times,end_times]).T, np.asarray(data)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ret manglende bindestreg i både de observeret sætning og de annoteret sætninger\n",
    "\n",
    "Grundet en fejl i præprocessering, er fx 'Europa-Parlementet' blevet til 'EuropaParlementet\". Det skal rettes.\n",
    "\n",
    "Derudover skal program1, program2, ... rettes til deres programID i stedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Path(save_loc, 'sample_programs-mbindestreg.pickle').open('rb') as f:\n",
    "    sample_dict_mbind = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLeadingSpace(s,start_idx):\n",
    "            # Finds first leading space before index \"start_idx\" in s\n",
    "            if start_idx < 0:\n",
    "                return 0\n",
    "            elif s[start_idx] is ' ' :\n",
    "                return start_idx+1\n",
    "            else:\n",
    "                return getLeadingSpace(s,start_idx-1)\n",
    "\n",
    "def getTailingSpace(s,end_idx):\n",
    "    # Finds first trailing space after index \"end_idx\" in s\n",
    "    if end_idx >= len(s):\n",
    "        return len(s)\n",
    "    elif s[end_idx] is ' ' or end_idx == len(s):\n",
    "        return end_idx\n",
    "    else:\n",
    "        return getTailingSpace(s,end_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2-udledning.\n",
      "\n",
      "vi kan gøre noget ved der dækker det lidt over halvdelen.\n",
      "\n",
      "De dækker 5-6% i alt.\n",
      "\n",
      "\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2-udledning. Af den del, vi kan gøre noget ved der dækker det lidt over halvdelen. Det er der, nøglen ligger. Jo, Svend. Husholdningerne er også med. Nej. De dækker 5-6% i alt.\n",
      "\n",
      "[[0, 143], [156, 213], [286, 307]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the fake program names to real program ids\n",
    "bugged_programs = ['program1','program2','program3','program4','program5',\\\n",
    "                  'program6','program7', 'program8', 'program9', 'program10']\n",
    "\n",
    "actual_programID = ['7308025','2294023','2315222','2337314','2359717',\\\n",
    "                  '2304494','2348260', '3411204', '3570949', '3662558']\n",
    "program_mapping = dict(zip(bugged_programs, actual_programID))\n",
    "\n",
    "## FIX inconsistencies related to the inclusion of \"-\" in the paragraphs\n",
    "for program in bugged_programs: # Fix each of the bugged programs\n",
    "    \n",
    "    idx_X = np.where(X[:,2] == program)[0] #Index in X\n",
    "    \n",
    "    for elem in range(idx_X.shape[0]): # For each paragraph\n",
    "        \n",
    "        X[idx_X[elem],2] = program_mapping[program]\n",
    "        \n",
    "        para_bugged = X[idx_X[elem], 4]\n",
    "        para_true = sample_dict_mbind[program]['sentences'][elem]\n",
    "        # Replace the bugged sentence with the corrected one\n",
    "        X[idx_X[elem], 4] = para_true\n",
    "        \n",
    "        \n",
    "        if X[idx_X[elem],6]: # If there is a claim\n",
    "            #print(X[idx_X[elem],5])\n",
    "            \n",
    "            if len(X[idx_X[elem],5]) == 1:\n",
    "                start_id = X[idx_X[elem],5][0][0]\n",
    "                end_id = X[idx_X[elem],5][0][1]\n",
    "                \n",
    "                claim_idx = [getLeadingSpace(para_true,start_id),\\\n",
    "                             getTailingSpace(para_true,end_id)]\n",
    "                \n",
    "                X[idx_X[elem],5][0] = claim_idx\n",
    "            else:\n",
    "                claim_idx = []\n",
    "                \n",
    "                print('Found:\\n%s' %X[idx_X[elem],6])\n",
    "                print(X[idx_X[elem],5])\n",
    "                \n",
    "                for c in range(len(X[idx_X[elem],5])):\n",
    "                    \n",
    "                    start_id = X[idx_X[elem],5][c][0]\n",
    "                    end_id = X[idx_X[elem],5][c][1]\n",
    "\n",
    "                    claim_idx.append([getLeadingSpace(para_true,start_id),\\\n",
    "                                 getTailingSpace(para_true,end_id)])\n",
    "                    \n",
    "                for idx in claim_idx:\n",
    "                    print(para_true[idx[0]:idx[1]]+'\\n')\n",
    "\n",
    "                X[idx_X[elem],5] = claim_idx\n",
    "                \n",
    "                print()\n",
    "                print(para_true)\n",
    "                \n",
    "                print()\n",
    "                print(claim_idx)\n",
    "                print()\n",
    "            \n",
    "        # Replace the bugged claim with the corrected one\n",
    "        # Correct the claim index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_claims = np.where([elem is not None for elem in X[:,6]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two claims where actually just mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An errornous annotation\n",
      "['20:09:03:23' '20:09:10:03' '7308025' 87\n",
      " 'Man diskuterer, om DONG skal privatiseres eller styres offentligt.'\n",
      " list([[4, 15]]) 'diskuterer,']\n",
      "The claim is removed\n",
      "['20:09:03:23' '20:09:10:03' '7308025' 87\n",
      " 'Man diskuterer, om DONG skal privatiseres eller styres offentligt.' None\n",
      " None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_claim_idx = np.where(X[:,2]=='7308025')[0][87-1]\n",
    "\n",
    "if X[remove_claim_idx,6]:\n",
    "    print('An errornous annotation')\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print('The claim is removed')\n",
    "    assert('diskuterer,'==X[remove_claim_idx,6])\n",
    "    X[remove_claim_idx,5] = None\n",
    "    X[remove_claim_idx,6] = None\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print()\n",
    "else:\n",
    "    print('No claim to remove\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An errornous annotation\n",
      "['00:13:13:24' '00:13:22:19' '3411204' 120\n",
      " 'For et år siden sagde Fogh, at vi ville komme til at mangle hænder. Jeg sagde, ledigheden ville stige. Fogh er her ikke mere.'\n",
      " list([[79, 89]]) 'ledigheden']\n",
      "The claim is removed\n",
      "['00:13:13:24' '00:13:22:19' '3411204' 120\n",
      " 'For et år siden sagde Fogh, at vi ville komme til at mangle hænder. Jeg sagde, ledigheden ville stige. Fogh er her ikke mere.'\n",
      " None None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_claim_idx = np.where(X[:,2]=='3411204')[0][120-1]\n",
    "\n",
    "if X[remove_claim_idx,6]:\n",
    "    print('An errornous annotation')\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print('The claim is removed')\n",
    "    assert('ledigheden'==X[remove_claim_idx,6])\n",
    "    X[remove_claim_idx,5] = None\n",
    "    X[remove_claim_idx,6] = None\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print()\n",
    "else:\n",
    "    print('No claim to remove\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "description = \"\"\"Claim detection in the television program DR Debatten\n",
    "\n",
    "In several Debatten programs interesting/relevant claims were annotated by DR.\n",
    "All sentences (or paragraphs) of these programs were extracted and marked as containing a\n",
    "claim or not. The claims themselves are also extracted.\n",
    "\n",
    "Currently the data contains N={:d} sentences from {:d} programs. The data is represented\n",
    "as a matrix of size N x M, where M is a number of attributes for each sentence.\n",
    "\n",
    "These attributes are:\n",
    "    'start time'      The start time of a sentence/paragraph (h:m:s:ms)\n",
    "    'end time'        The end time of a sentence/paragraph\n",
    "    'program_id'      Indicates which Debatten program is the origin of the sentence\n",
    "    'sentences_id'    Indicates which sentence in the program it is\n",
    "                      (ordered from 1 to 2 to .. to the last sentence)\n",
    "    'sentence'        A string with the full sentence/paragraph\n",
    "    'claim_idx'       [start, end]-index of the claim (in the sentence)\n",
    "    'claim'           A string with the claim\n",
    "\n",
    "The data is available as a .csv file and .pickle file (python3).\n",
    "\n",
    "Copyright(R): This data is made available in connection with the course \"02456 Deep Learning\" \n",
    "at the Technical University of Denmark, during the Fall 2017. Redistribution or commercial use\n",
    "of the dataset is not allowed without prior agreement.\n",
    "\n",
    "\n",
    "-------- Python3 Example: Load data, transform to Bag-of-Words and fit a logistic regression ----\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "with open(\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "X = data['data'][:,4]\n",
    "y = data['data'][:,6]\n",
    "\n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(len(X))])       \n",
    "\n",
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "confusion_matrix(y, ypred)\n",
    "\n",
    "\"\"\".format(N,len(processed_programs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(save_loc, 'readme_sample_programs.txt').open('w') as f:\n",
    "    f.write(description)\n",
    "\n",
    "# Add features as top row\n",
    "Y = np.concatenate((np.asarray(features).reshape(1,-1), X), axis=0);\n",
    "\n",
    "with Path(save_loc, \"data_matrix_sample_programs.csv\").open(\"w\", newline=\"\\n\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file, delimiter=\",\")\n",
    "    for row in Y:\n",
    "        # row = [str(val).encode(\"utf-8\") for val in row]\n",
    "        # print(row[4])\n",
    "        # row[4] = row[4].encode(\"utf-8\")\n",
    "        # print(\"   \", row[4])\n",
    "        # print(\"   \", row[4].decode())\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with Path(save_loc, \"data_matrix_sample_programs.pickle\").open('wb') as f:\n",
    "        pickle.dump(dict(zip(['data','features'],[X, features])), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
