{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../util/DebattenAnnotatedDatacleaner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../util/DebattenAnnotatedDatacleaner.py\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class DebattenAnnotatedDatacleaner:\n",
    "    \"\"\"\n",
    "    Takes the annotated programs, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialises class and input, output locations\n",
    "    def __init__(self, loc_ann=[], loc_out=[]):\n",
    "        self.loc_ann_subtitles = loc_ann\n",
    "        self.loc_out_subtitles = loc_out\n",
    "    \n",
    "    def setAnnotatedFilesLocation(self, new_loc):\n",
    "        self.loc_ann_subtitles = new_loc\n",
    "        \n",
    "    def setOutputFilesLocation(self, new_loc):\n",
    "        self.loc_out_subtitles = new_loc\n",
    "    \n",
    "    def getFileLocation(self, disp=True):\n",
    "        \n",
    "        if disp:\n",
    "            if not self.loc_ann_subtitles:\n",
    "                print('Annotated subtitles are not specified!')\n",
    "            else:\n",
    "                print('Annotated subtitles are loaded from \"{:s}\"'.format(self.loc_ann_subtitles))\n",
    "\n",
    "            if not self.loc_out_subtitles:\n",
    "                print('Save location is not specified!')\n",
    "            else:\n",
    "                print('Save location is \"{:s}\"'.format(self.loc_out_subtitles))\n",
    "       \n",
    "        return self.loc_ann_subtitles, self.loc_out_subtitles\n",
    "    \n",
    "    def getFilePaths(self):\n",
    "        files = os.listdir(self.loc_ann_subtitles)\n",
    "        return [self.loc_ann_subtitles+f for f in files]\n",
    "    \n",
    "    \n",
    "    def getProgramAndSentences(self,f_path):\n",
    "        \"\"\"Gets the program id, sentences id and sentences from a document\"\"\"\n",
    "        with open(f_path,'r') as f:\n",
    "            doc = f.read()\n",
    "\n",
    "        #Find program id\n",
    "        m_program_id = re.compile('program[\\d]+')\n",
    "        m = re.search(m_program_id, doc)\n",
    "        program_id = m.group()\n",
    "\n",
    "        \n",
    "        sentences = doc.split('<p ')\n",
    "        m_sentence_id = re.compile('id=\"[\\d]+\">')\n",
    "\n",
    "        # Finds the sentence ids and removes html stuff from the begining of each sentence\n",
    "        sentences_id = []\n",
    "        for i in range(len(sentences)):\n",
    "            match = re.search(m_sentence_id, sentences[i])\n",
    "            if not match:\n",
    "                sentences[i] = None\n",
    "            else:\n",
    "                sentences_id.append(int(match.group()[4:-2]))\n",
    "\n",
    "                start_from = sentences[i].find('>')+1\n",
    "                sentences[i] = sentences[i][start_from:]\n",
    "\n",
    "        sentences = list(filter(None, sentences)) # Remove None elements\n",
    "        assert(len(sentences)==len(sentences_id))\n",
    "\n",
    "        return program_id, sentences_id, sentences\n",
    "    \n",
    "    # Finds highligted text including its surrounding patttern\n",
    "    def findHighlights(self,s):\n",
    "        m_highlight = re.compile('<span id=\"highlight[\"\\w\\d ]+class=\"highlight[\\w\"]+>[\\w\\d. ,!?%]+</span>')\n",
    "        return re.findall(m_highlight, s)\n",
    "    \n",
    "    # Extracts highlighted text only\n",
    "    def extractHighlights(self, s_matches):#Extracted the text highlighted\n",
    "        m_high_text = re.compile('\">[\\w\\d ,.!?%]+</')\n",
    "        high_text = [re.findall(m_high_text, s_matches[i])[0][2:-2] for i in range(len(s_matches))]\n",
    "        return [s.lstrip().rstrip() for s in high_text]\n",
    "    \n",
    "    # Removes html tags (and crap) from the string.\n",
    "    def cleanSentence(self, s, disp=False):\n",
    "\n",
    "        m_crap = re.compile('<[\\w\\d \"=/]+>')\n",
    "        s_crap_free = s\n",
    "        for pattern in re.findall(m_crap, s): \n",
    "            if disp: print(pattern)\n",
    "            s_crap_free = s_crap_free.replace(pattern,'')\n",
    "\n",
    "        #s_crap_free = re.sub('id=\"[\\d]+\">','',s_crap_free) # only during dev\n",
    "\n",
    "        s_crap_free = s_crap_free.replace('\\t',' ') # removes tabs\n",
    "        s_crap_free = re.sub(' +',' ', s_crap_free) # removes excess spaces\n",
    "        return s_crap_free.lstrip().rstrip()\n",
    "\n",
    "    def getHighlight_indices(self,s,s_highlighted):\n",
    "        \n",
    "        # Two heuristic for correcting partially highlighted words.\n",
    "        def getLeadingSpace(s,start_idx):\n",
    "            # Finds first leading space before index \"start_idx\" in s\n",
    "            if start_idx < 0:\n",
    "                return 0\n",
    "            elif s[start_idx] is ' ' :\n",
    "                return start_idx+1\n",
    "            else:\n",
    "                return getLeadingSpace(s,start_idx-1)\n",
    "\n",
    "        def getTailingSpace(s,end_idx):\n",
    "            # Finds first trailing space after index \"end_idx\" in s\n",
    "            if end_idx >= len(s):\n",
    "                return len(s)\n",
    "            elif s[end_idx] is ' ' or end_idx == len(s)-1:\n",
    "                return end_idx\n",
    "            else:\n",
    "                return getTailingSpace(s,end_idx+1)\n",
    "        \n",
    "        # Find the indicies of highlighted words\n",
    "        indices = []\n",
    "        # Get matched indices\n",
    "        for m in s_highlighted:\n",
    "            m_pattern = re.compile(m)\n",
    "            match = re.search(m_pattern, s)\n",
    "            if match:\n",
    "                indices.append([getLeadingSpace(s, match.start()), \n",
    "                                getTailingSpace(s, match.end())])\n",
    "            else:\n",
    "                print(match)\n",
    "                print(m)\n",
    "                print(s_highlighted)\n",
    "                print(s+'\\n')\n",
    "                \n",
    "        #print('\\n\\n')\n",
    "        return indices\n",
    "    \n",
    "    def getCleanedProgramSentences(self, sentences): \n",
    "        sentences_processed = [None]*len(sentences)\n",
    "        sentences_highlight = [None]*len(sentences)\n",
    "        sentences_highlight_ind = [None]*len(sentences)\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            sen = sentences[i]\n",
    "            raw_highlights = self.findHighlights(sen)\n",
    "            text_highlights = self.extractHighlights(raw_highlights)\n",
    "            \n",
    "            #Crap free verion\n",
    "            sentences_processed[i] = self.cleanSentence(sen)\n",
    "            #print('cleaned: '+sentences_processed[i])\n",
    "            indices_highlights = self.getHighlight_indices(sentences_processed[i], \n",
    "                                                                         text_highlights)\n",
    "            sentences_highlight_ind[i] = indices_highlights\n",
    "            \n",
    "            for idx in indices_highlights:\n",
    "                if sentences_highlight[i]:\n",
    "                     sentences_highlight[i] = sentences_highlight[i]+ ' [new claim]: '\\\n",
    "                                              +sentences_processed[i][idx[0]:idx[1]]\n",
    "                else:\n",
    "                    sentences_highlight[i] = sentences_processed[i][idx[0]:idx[1]]\n",
    "            \n",
    "            \n",
    "        return sentences_processed, sentences_highlight, sentences_highlight_ind\n",
    "    \n",
    "    def getAllCleanedProgramSentences(self,disp=False):\n",
    "        file_paths = self.getFilePaths()\n",
    "\n",
    "        all_program_id = [None]*len(file_paths)\n",
    "        all_sentences = [None]*len(file_paths)\n",
    "        all_sentences_id = [None]*len(file_paths)\n",
    "        all_highlights = [None]*len(file_paths)\n",
    "        all_highlights_ind = [None]*len(file_paths)\n",
    "        \n",
    "        total_claims = 0;\n",
    "        total_sentences = 0;\n",
    "        \n",
    "        for f in range(len(file_paths)):\n",
    "            all_program_id[f], all_sentences_id[f], sentences = \\\n",
    "                        self.getProgramAndSentences(file_paths[f])\n",
    "            if disp: print('Program id {:s}'.format(all_program_id[f]))\n",
    "            \n",
    "            all_sentences[f], all_highlights[f], all_highlights_ind[f] = \\\n",
    "                        self.getCleanedProgramSentences(sentences)\n",
    "            \n",
    "            num_claims = len(list(filter(None,all_highlights[f])))\n",
    "            if disp: print('\\tThere were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(num_claims\\\n",
    "                    ,len(sentences), num_claims/float(len(sentences))*100))\n",
    "                \n",
    "            total_claims = total_claims+num_claims\n",
    "            total_sentences = total_sentences + len(sentences)\n",
    "            \n",
    "        if disp: print('\\nIn total there were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(total_claims\\\n",
    "                , total_sentences, total_claims/float(total_sentences)*100))\n",
    "        \n",
    "        # ...\n",
    "        labels = ['program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n",
    "        \n",
    "        data = [ [None]*len(labels) for i in range(total_sentences)]\n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "        for p in range(len(all_program_id)):\n",
    "            \n",
    "            for si in range(len(all_sentences[p])):\n",
    "                data[i][0] = all_program_id[p]\n",
    "                \n",
    "                data[i][1] = all_sentences_id[p][si]\n",
    "                data[i][2] = all_sentences[p][si]\n",
    "                \n",
    "                if len(all_highlights_ind[p][si]) == 1:\n",
    "                    data[i][3] = all_highlights_ind[p][si]\n",
    "                    data[i][4] = all_highlights[p][si]\n",
    "                    \n",
    "                elif all_highlights_ind[p][si]:\n",
    "                    print('HELP')\n",
    "                    print(all_program_id[p])\n",
    "                    print(all_highlights_ind[p][si])\n",
    "                    print(all_highlights[p][si])\n",
    "                \n",
    "                i = i+1\n",
    "            \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../util\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from DebattenAnnotatedDatacleaner import DebattenAnnotatedDatacleaner\n",
    "\n",
    "loc_ann_data = '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/'\n",
    "annotatedData = DebattenAnnotatedDatacleaner(loc_ann_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id is program1\n"
     ]
    }
   ],
   "source": [
    "file_paths = annotatedData.getFilePaths()\n",
    "\n",
    "program_id, sentences_id, sentences = annotatedData.getProgramAndSentences(file_paths[2])\n",
    "print('Id is '+program_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program10.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program7.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program1.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program9.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program5.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program4.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program3.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program6.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program8.txt',\n",
       " '/home/jehi/Dropbox/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles/program2.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <span id=\"highlight0\" class=\"highlightGreen\"><span id=\"highlight1\" class=\"highlightYellow\">Programmet blev debatteret af flere end 32.000 danskere på nettet.</span></span> </p>\\t Finanseksperter vurderer at d<span id=\"highlight3\" class=\"highlight\">e kan tjene 100% af den investering, de laver, på få år.</span> </p>\\t'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sentences[6]+sentences[47]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<span id=\"highlight1\" class=\"highlightYellow\">Programmet blev debatteret af flere end 32.000 danskere på nettet.</span>', '<span id=\"highlight3\" class=\"highlight\">e kan tjene 100% af den investering, de laver, på få år.</span>']\n"
     ]
    }
   ],
   "source": [
    "match_high = annotatedData.findHighlights(s)\n",
    "print(match_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Programmet blev debatteret af flere end 32.000 danskere på nettet.', 'e kan tjene 100% af den investering, de laver, på få år.']\n"
     ]
    }
   ],
   "source": [
    "match_text_high = annotatedData.extractHighlights(match_high)\n",
    "print(match_text_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span id=\"highlight0\" class=\"highlightGreen\">\n",
      "<span id=\"highlight1\" class=\"highlightYellow\">\n",
      "</span>\n",
      "</span>\n",
      "</p>\n",
      "<span id=\"highlight3\" class=\"highlight\">\n",
      "</span>\n",
      "</p>\n",
      "\n",
      "\n",
      "Programmet blev debatteret af flere end 32.000 danskere på nettet. Finanseksperter vurderer at de kan tjene 100% af den investering, de laver, på få år.\n"
     ]
    }
   ],
   "source": [
    "s_crap_free = annotatedData.cleanSentence(s, disp=True)\n",
    "print('\\n\\n' +s_crap_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 66], [95, 152]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotatedData.getHighlight_indices(s_crap_free, match_text_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program id program10\n",
      "\tThere were 7 claims out of 231 sentences (3.03%)\n",
      "Program id program7\n",
      "\tThere were 12 claims out of 307 sentences (3.91%)\n",
      "Program id program1\n",
      "\tThere were 46 claims out of 516 sentences (8.91%)\n",
      "Program id program9\n",
      "\tThere were 22 claims out of 307 sentences (7.17%)\n",
      "Program id program5\n",
      "\tThere were 42 claims out of 309 sentences (13.59%)\n",
      "Program id program4\n",
      "\tThere were 30 claims out of 317 sentences (9.46%)\n",
      "Program id program3\n",
      "\tThere were 20 claims out of 442 sentences (4.52%)\n",
      "Program id program6\n",
      "\tThere were 22 claims out of 334 sentences (6.59%)\n",
      "Program id program8\n",
      "\tThere were 32 claims out of 406 sentences (7.88%)\n",
      "Program id program2\n",
      "\tThere were 33 claims out of 320 sentences (10.31%)\n",
      "\n",
      "In total there were 266 claims out of 3489 sentences (7.62%)\n",
      "HELP\n",
      "program5\n",
      "[[0, 71], [72, 112]]\n",
      "Det Europæiske Energiagentur har gjort op at Danmark er et af de lande, [new claim]: der er længst væk fra at opfylde målene.\n",
      "HELP\n",
      "program5\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "HELP\n",
      "program4\n",
      "[[0, 57], [58, 136]]\n",
      "Mange af problemerne er skabt af den tidligere præsident, [new claim]: altså Bush fordi han har skubbet uro ind i områder, der før var under kontrol.\n",
      "HELP\n",
      "program3\n",
      "[[0, 18], [19, 59]]\n",
      "Jeg kan frygte, at [new claim]: DR1 bliver en slags TV2 i middelmådighed\n",
      "HELP\n",
      "program6\n",
      "[[0, 60], [61, 105]]\n",
      "Efter syv år med fremgang er virkeligheden vendt for Venstre [new claim]: som får historiske smæk i meningsmålingerne.\n",
      "HELP\n",
      "program6\n",
      "[[0, 9], [10, 37]]\n",
      "Jeg tror, [new claim]: vi får en stigende ledighed\n"
     ]
    }
   ],
   "source": [
    "data, labels = annotatedData.getAllCleanedProgramSentences(disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Combine timestamps from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['program8', 'program9', 'program2', 'program4', 'program7', 'program6', 'program5', 'program10', 'program1', 'program3'])\n",
      "\n",
      "dict_keys(['start time', 'end time', 'sentences'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "save_loc = '/home/jehi/Dropbox/DTU/DeepFactData/annotated/'\n",
    "\n",
    "with open(save_loc+'sample_programs.pickle', 'rb') as f:\n",
    "    sample_dict = pickle.load(f)\n",
    "    \n",
    "print(sample_dict.keys())\n",
    "print()\n",
    "print(sample_dict['program1'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start time', 'end time', 'program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n"
     ]
    }
   ],
   "source": [
    "N = len(data) # Observations\n",
    "features = ['start time', 'end time']\n",
    "[features.append(lab) for lab in labels]\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Get the time from the processed data\n",
    "start_times = []\n",
    "end_times = []\n",
    "processed_programs = []\n",
    "\n",
    "for i in range(N):  \n",
    "    pro_id = data[i][0]\n",
    "    \n",
    "    # Note sample_dict[pro_id]['start time'] is a list of start times for all sentences\n",
    "    if pro_id not in processed_programs:\n",
    "        [start_times.append(t) for t in sample_dict[pro_id]['start time']]\n",
    "        [end_times.append(t) for t in sample_dict[pro_id]['end time']]\n",
    "        \n",
    "        processed_programs.append(pro_id)\n",
    "# Sanity check        \n",
    "assert(len(start_times) == len(end_times))\n",
    "assert(len(start_times) == N)\n",
    "\n",
    "#Concat data\n",
    "X = np.concatenate((np.asarray([start_times,end_times]).T, np.asarray(data)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "description = \"\"\"Claim detection in the television program DR Debatten\n",
    "\n",
    "In several Debatten programs interesting/relevant claims were annotated by DR.\n",
    "All sentences (or paragraphs) of these programs were extracted and marked as containing a\n",
    "claim or not. The claims themselves are also extracted.\n",
    "\n",
    "Currently the data contains N={:d} sentences from {:d} programs. The data is represented\n",
    "as a matrix of size N x M, where M is a number of attributes for each sentence.\n",
    "\n",
    "These attributes are:\n",
    "    'start time'      The start time of a sentence/paragraph (h:m:s:ms)\n",
    "    'end time'        The end time of a sentence/paragraph\n",
    "    'program_id'      Indicates which Debatten program is the origin of the sentence\n",
    "    'sentences_id'    Indicates which sentence in the program it is\n",
    "                      (ordered from 1 to 2 to .. to the last sentence)\n",
    "    'sentence'        A string with the full sentence/paragraph\n",
    "    'claim_idx'       [start, end]-index of the claim (in the sentence)\n",
    "    'claim'           A string with the claim\n",
    "\n",
    "The data is available as a .csv file and .pickle file (python3).\n",
    "\n",
    "Copyright(R): This data is made available in connection with the course \"02456 Deep Learning\" \n",
    "at the Technical University of Denmark, during the Fall 2017. Redistribution or commercial use\n",
    "of the dataset is not allowed without prior agreement.\n",
    "\n",
    "\n",
    "-------- Python3 Example: Load data, transform to Bag-of-Words and fit a logistic regression ----\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "with open(\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "X = data['data'][:,4]\n",
    "y = data['data'][:,6]\n",
    "\n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(len(X))])       \n",
    "\n",
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "confusion_matrix(y, ypred)\n",
    "\n",
    "\"\"\".format(N,len(processed_programs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(save_loc+'readme_sample_programs.txt','w') as f:\n",
    "    f.write(description)\n",
    "\n",
    "#Add features as top row\n",
    "Y = np.concatenate((np.asarray(features).reshape(1,-1),X), axis=0);\n",
    "np.savetxt(save_loc+\"data_matrix_sample_programs.csv\", Y , delimiter=\",\", fmt='%s')\n",
    "\n",
    "with open(save_loc+\"data_matrix_sample_programs.pickle\",'wb') as f:\n",
    "        pickle.dump(dict(zip(['data','features'],[X, features])), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
