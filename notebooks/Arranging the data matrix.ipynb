{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import csv\n",
    "from pathlib import Path\n",
    "sys.path.append(\"..\")\n",
    "from project_paths import ProjectPaths\n",
    "ProjectPaths.set_path_to_repository(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Available project.paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir                                : ../../data\n",
      "dr_detektor_automatic_fact_checking_dir : ../../data/DRDetektorAutomaticFactChecking\n",
      "annotated_subtitles                     : ../../data/DRDetektorAutomaticFactChecking/annotatorProgram/annotatedSubtitles\n",
      "deep_fact_dir                           : ../../data/DeepFactData\n",
      "annotated                               : ../../data/DeepFactData/annotated\n",
      "data_matrix_path                        : ../../data/DeepFactData/annotated/data_matrix_sample_programs.csv\n",
      "nlp_data_dir                            : ../../data/DeepFactData/nlp_data\n",
      "embeddings_file                         : ../../data/DeepFactData/nlp_data/embeddings.csv\n",
      "pos_tags_file                           : ../../data/DeepFactData/nlp_data/pos_tags.csv\n",
      "speller_dir                             : ../../data/DeepFactData/spelling_model\n",
      "speller_char_vocab_file                 : ../../data/DeepFactData/spelling_model/char_embedding.json\n",
      "speller_encoder_checkpoint_file         : ../../data/DeepFactData/spelling_model/checkpoint/speller_encode.ckpt\n",
      "speller_results_file                    : ../../data/DeepFactData/spelling_model/results.json\n",
      "speller_translator_file                 : ../../data/DeepFactData/spelling_model/string_translator.json\n",
      "results                                 : ../../data/results\n"
     ]
    }
   ],
   "source": [
    "ProjectPaths.print_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_loc = ProjectPaths.annotated\n",
    "loc_ann_data = ProjectPaths.annotated_subtitles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Cleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class DebattenAnnotatedDatacleaner:\n",
    "    \"\"\"\n",
    "    Takes the annotated programs, ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialises class and input, output locations\n",
    "    def __init__(self, loc_ann=None, loc_out=None):\n",
    "        self.loc_ann_subtitles = loc_ann\n",
    "        self.loc_out_subtitles = loc_out\n",
    "    \n",
    "    def setAnnotatedFilesLocation(self, new_loc):\n",
    "        self.loc_ann_subtitles = new_loc\n",
    "        \n",
    "    def setOutputFilesLocation(self, new_loc):\n",
    "        self.loc_out_subtitles = new_loc\n",
    "    \n",
    "    def getFileLocation(self, disp=True):\n",
    "        \n",
    "        if disp:\n",
    "            if self.loc_ann_subtitles is None:\n",
    "                print('Annotated subtitles are not specified!')\n",
    "            else:\n",
    "                print('Annotated subtitles are loaded from \"{:s}\"'.format(self.loc_ann_subtitles))\n",
    "\n",
    "            if self.loc_out_subtitles is None:\n",
    "                print('Save location is not specified!')\n",
    "            else:\n",
    "                print('Save location is \"{:s}\"'.format(self.loc_out_subtitles))\n",
    "       \n",
    "        return self.loc_ann_subtitles, self.loc_out_subtitles\n",
    "    \n",
    "    def getFilePaths(self):\n",
    "        files = self.loc_ann_subtitles.glob(\"*.txt\")\n",
    "        return list(files)\n",
    "    \n",
    "    def getProgramAndSentences(self, f_path):\n",
    "        \"\"\"Gets the program id, sentences id and sentences from a document\"\"\"\n",
    "        with f_path.open('r', encoding=\"utf-8\") as f:\n",
    "            doc = f.read()\n",
    "\n",
    "        #Find program id\n",
    "        #m_program_id = re.compile('program[\\d]+')\n",
    "        m_program_id = re.compile('ram[ \\d]+\"')\n",
    "        #print('\\n ')\n",
    "        \n",
    "        m = re.search(m_program_id, doc)\n",
    "        program_id = m.group()\n",
    "        program_id = program_id[3:-1].strip()\n",
    "\n",
    "        sentences = doc.split('<p ')\n",
    "        m_sentence_id = re.compile('id=\"[\\d]+\">')\n",
    "\n",
    "        # Finds the sentence ids and removes html stuff from the begining of each sentence\n",
    "        sentences_id = []\n",
    "        for i in range(len(sentences)):\n",
    "            match = re.search(m_sentence_id, sentences[i])\n",
    "            if not match:\n",
    "                sentences[i] = None\n",
    "            else:\n",
    "                sentences_id.append(int(match.group()[4:-2]))\n",
    "\n",
    "                start_from = sentences[i].find('>')+1\n",
    "                sentences[i] = sentences[i][start_from:]\n",
    "\n",
    "        sentences = list(filter(None, sentences)) # Remove None elements\n",
    "        assert(len(sentences)==len(sentences_id))\n",
    "\n",
    "        return program_id, sentences_id, sentences\n",
    "    \n",
    "    # Finds highligted text including its surrounding patttern\n",
    "    def findHighlights(self,s):\n",
    "        m_highlight = re.compile('<span id=\"highlight[\"\\w\\d ]+class=\"highlight[\\w\"]+>[\\w\\d. ,!?%]+</span>')\n",
    "        return re.findall(m_highlight, s)\n",
    "    \n",
    "    # Extracts highlighted text only\n",
    "    def extractHighlights(self, s_matches):#Extracted the text highlighted\n",
    "        m_high_text = re.compile('\">[\\w\\d ,.!?%]+</')\n",
    "        high_text = [re.findall(m_high_text, s_matches[i])[0][2:-2] for i in range(len(s_matches))]\n",
    "        return [s.lstrip().rstrip() for s in high_text]\n",
    "    \n",
    "    # Removes html tags (and crap) from the string.\n",
    "    def cleanSentence(self, s, disp=False):\n",
    "\n",
    "        m_crap = re.compile('<[\\w\\d \"=/]+>')\n",
    "        s_crap_free = s\n",
    "        for pattern in re.findall(m_crap, s): \n",
    "            if disp: print(pattern)\n",
    "            s_crap_free = s_crap_free.replace(pattern,'')\n",
    "\n",
    "        #s_crap_free = re.sub('id=\"[\\d]+\">','',s_crap_free) # only during dev\n",
    "\n",
    "        s_crap_free = s_crap_free.replace('\\t',' ') # removes tabs\n",
    "        s_crap_free = re.sub(' +',' ', s_crap_free) # removes excess spaces\n",
    "        return s_crap_free.lstrip().rstrip()\n",
    "\n",
    "    def getHighlight_indices(self,s,s_highlighted):\n",
    "        \n",
    "        # Two heuristic for correcting partially highlighted words.\n",
    "        def getLeadingSpace(s,start_idx):\n",
    "            # Finds first leading space before index \"start_idx\" in s\n",
    "            if start_idx < 0:\n",
    "                return 0\n",
    "            elif s[start_idx] is ' ' :\n",
    "                return start_idx+1\n",
    "            else:\n",
    "                return getLeadingSpace(s,start_idx-1)\n",
    "\n",
    "        def getTailingSpace(s,end_idx):\n",
    "            # Finds first trailing space after index \"end_idx\" in s\n",
    "            if end_idx >= len(s):\n",
    "                return len(s)\n",
    "            elif s[end_idx] is ' ' or end_idx == len(s):\n",
    "                return end_idx\n",
    "            else:\n",
    "                return getTailingSpace(s,end_idx+1)\n",
    "        \n",
    "        # Find the indicies of highlighted words\n",
    "        indices = []\n",
    "        # Get matched indices\n",
    "        for m in s_highlighted:\n",
    "            \n",
    "            if m is not '?':\n",
    "\n",
    "                m_pattern = re.compile(m)\n",
    "                match = re.search(m_pattern, s)\n",
    "                #print(type(match))\n",
    "                if match:\n",
    "                    indices.append([getLeadingSpace(s, match.start()), \n",
    "                                    getTailingSpace(s, match.end())])\n",
    "                else:\n",
    "\n",
    "                    print(match)\n",
    "                    print(m)\n",
    "                    print(s_highlighted)\n",
    "                    print(s+'\\n')\n",
    "            else:\n",
    "                print('Annotation bug. A single question mark was annotated..')\n",
    "            \n",
    "                \n",
    "        #print('\\n\\n')\n",
    "        return indices\n",
    "    \n",
    "    def getCleanedProgramSentences(self, sentences): \n",
    "        sentences_processed = [None]*len(sentences)\n",
    "        sentences_highlight = [None]*len(sentences)\n",
    "        sentences_highlight_ind = [None]*len(sentences)\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            sen = sentences[i]\n",
    "            raw_highlights = self.findHighlights(sen)\n",
    "            text_highlights = self.extractHighlights(raw_highlights)\n",
    "            \n",
    "            #Crap free verion\n",
    "            sentences_processed[i] = self.cleanSentence(sen)\n",
    "            #print('cleaned: '+sentences_processed[i])\n",
    "            indices_highlights = self.getHighlight_indices(sentences_processed[i], \n",
    "                                                                         text_highlights)\n",
    "            sentences_highlight_ind[i] = indices_highlights\n",
    "            \n",
    "            for idx in indices_highlights:\n",
    "                if sentences_highlight[i]:\n",
    "                     sentences_highlight[i] = sentences_highlight[i]+ ' [new claim]: '\\\n",
    "                                              +sentences_processed[i][idx[0]:idx[1]]\n",
    "                else:\n",
    "                    sentences_highlight[i] = sentences_processed[i][idx[0]:idx[1]]\n",
    "            \n",
    "            \n",
    "        return sentences_processed, sentences_highlight, sentences_highlight_ind\n",
    "    \n",
    "    # EXPERIMENTAL!!! Processing multi-claim paragraphs\n",
    "    def processMultiClaim(self,s,idx):\n",
    "        merge_claims = []\n",
    "        for c in range(len(idx)-1):\n",
    "            if idx[c][1]-idx[c+1][0] >= -1: #It is the same claim\n",
    "                merge_claims.append(True)\n",
    "            else:\n",
    "                merge_claims.append(False)\n",
    "\n",
    "        new_s = []\n",
    "        new_idx = []\n",
    "        for c in range(len(idx)-1):\n",
    "            if merge_claims[c]:\n",
    "                start_id = idx[c][0]\n",
    "                end_id = idx[c+1][1]\n",
    "                new_idx.append([start_id, end_id])\n",
    "                new_s.append(s[start_id:end_id])\n",
    "            else:\n",
    "                if c > 0:\n",
    "                    new_s.append(' [new claim]: ')\n",
    "\n",
    "                start_id = idx[c][0]\n",
    "                end_id = idx[c][1]\n",
    "                new_idx.append([start_id, end_id])\n",
    "                new_s.append(s[start_id:end_id])\n",
    "\n",
    "        if not merge_claims[-1]:\n",
    "\n",
    "            new_s.append(' [new claim]: ')\n",
    "\n",
    "            start_id = idx[-1][0]\n",
    "            end_id = idx[-1][1]\n",
    "            new_idx.append([start_id, end_id])\n",
    "            new_s.append(s[start_id:end_id])\n",
    "\n",
    "\n",
    "        new_s = ''.join(new_s)\n",
    "        return new_s, new_idx\n",
    "    \n",
    "    def getAllCleanedProgramSentences(self,disp=False):\n",
    "        file_paths = self.getFilePaths()\n",
    "\n",
    "        all_program_id = [None]*len(file_paths)\n",
    "        all_sentences = [None]*len(file_paths)\n",
    "        all_sentences_id = [None]*len(file_paths)\n",
    "        all_highlights = [None]*len(file_paths)\n",
    "        all_highlights_ind = [None]*len(file_paths)\n",
    "        \n",
    "        total_claims = 0;\n",
    "        total_sentences = 0;\n",
    "        \n",
    "        for f in range(len(file_paths)):\n",
    "            all_program_id[f], all_sentences_id[f], sentences = \\\n",
    "                        self.getProgramAndSentences(file_paths[f])\n",
    "            if disp: print('Program id {:s}'.format(all_program_id[f]))\n",
    "            \n",
    "            all_sentences[f], all_highlights[f], all_highlights_ind[f] = \\\n",
    "                        self.getCleanedProgramSentences(sentences)\n",
    "            \n",
    "            num_claims = len(list(filter(None,all_highlights[f])))\n",
    "            if disp: print('\\tThere were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(num_claims\\\n",
    "                    ,len(sentences), num_claims/float(len(sentences))*100))\n",
    "                \n",
    "            total_claims = total_claims+num_claims\n",
    "            total_sentences = total_sentences + len(sentences)\n",
    "            \n",
    "        if disp: print('\\nIn total there were {:d} claims out of {:d} sentences ({:2.2f}%)'.format(total_claims\\\n",
    "                , total_sentences, total_claims/float(total_sentences)*100))\n",
    "        \n",
    "        # ...\n",
    "        labels = ['program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n",
    "        \n",
    "        data = [ [None]*len(labels) for i in range(total_sentences)]\n",
    "        \n",
    "        \n",
    "        i = 0\n",
    "        for p in range(len(all_program_id)):\n",
    "            \n",
    "            for si in range(len(all_sentences[p])):\n",
    "                data[i][0] = all_program_id[p]\n",
    "                \n",
    "                data[i][1] = si+1#Sentence ID is not correct in all files#all_sentences_id[p][si]\n",
    "                data[i][2] = all_sentences[p][si]\n",
    "                \n",
    "                if len(all_highlights_ind[p][si]) == 1:\n",
    "                    data[i][3] = all_highlights_ind[p][si]\n",
    "                    data[i][4] = all_highlights[p][si]\n",
    "                    \n",
    "                elif all_highlights_ind[p][si]:\n",
    "                    \n",
    "                    print('HELP')\n",
    "                    print(all_program_id[p])\n",
    "                    #print(all_sentences[p][si])\n",
    "                    print(all_highlights_ind[p][si])\n",
    "                    print(all_highlights[p][si])\n",
    "                    new_s, new_idx = self.processMultiClaim(all_sentences[p][si],\\\n",
    "                                                      all_highlights_ind[p][si])\n",
    "                    \n",
    "                    print('Trying to handle this multi-claim, is the output correct?')\n",
    "                    print(new_idx)\n",
    "                    print(new_s)\n",
    "                    print()\n",
    "                    \n",
    "                    data[i][3] = new_idx\n",
    "                    data[i][4] = new_s\n",
    "                \n",
    "                i = i+1\n",
    "            \n",
    "        return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "annotatedData = DebattenAnnotatedDatacleaner(loc_ann_data)\n",
    "file_paths = annotatedData.getFilePaths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Test of file-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id is 10\n",
      "['<span id=\"highlight48\" class=\"highlight\">Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.</span>']\n",
      "['Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.']\n",
      "<span id=\"highlight48\" class=\"highlight\">\n",
      "</span>\n",
      "</p>\n",
      "\n",
      "\n",
      "Det er dem, der støtter ham. Så når de store lande vælger sådan en mand er det, fordi han står for deres synspunkter.\n",
      "[[29, 117]]\n"
     ]
    }
   ],
   "source": [
    "program_id, sentences_id, sentences = annotatedData.getProgramAndSentences(file_paths[1])\n",
    "print('Id is '+program_id)\n",
    "\n",
    "s = sentences[111]\n",
    "\n",
    "match_high = annotatedData.findHighlights(s)\n",
    "print(match_high)\n",
    "\n",
    "match_text_high = annotatedData.extractHighlights(match_high)\n",
    "print(match_text_high)\n",
    "\n",
    "s_crap_free = annotatedData.cleanSentence(s, disp=True)\n",
    "print('\\n\\n' +s_crap_free)\n",
    "\n",
    "print(annotatedData.getHighlight_indices(s_crap_free, match_text_high))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Process all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program id 8568658\n",
      "\tThere were 33 claims out of 523 sentences (6.31%)\n",
      "Program id 10\n",
      "\tThere were 7 claims out of 231 sentences (3.03%)\n",
      "Program id 8689224\n",
      "\tThere were 33 claims out of 541 sentences (6.10%)\n",
      "Program id 7\n",
      "\tThere were 12 claims out of 307 sentences (3.91%)\n",
      "Program id 1\n",
      "\tThere were 46 claims out of 516 sentences (8.91%)\n",
      "Program id 8720741\n",
      "\tThere were 36 claims out of 541 sentences (6.65%)\n",
      "Program id 9\n",
      "\tThere were 22 claims out of 307 sentences (7.17%)\n",
      "Program id 8567181\n",
      "\tThere were 29 claims out of 324 sentences (8.95%)\n",
      "Program id 5\n",
      "\tThere were 42 claims out of 309 sentences (13.59%)\n",
      "Program id 8568906\n",
      "\tThere were 28 claims out of 518 sentences (5.41%)\n",
      "Program id 4\n",
      "\tThere were 30 claims out of 317 sentences (9.46%)\n",
      "Program id 3\n",
      "\tThere were 20 claims out of 442 sentences (4.52%)\n",
      "Program id 9284846\n",
      "\tThere were 42 claims out of 577 sentences (7.28%)\n",
      "Program id 8665813\n",
      "\tThere were 32 claims out of 561 sentences (5.70%)\n",
      "Program id 6\n",
      "\tThere were 22 claims out of 334 sentences (6.59%)\n",
      "Program id 8635201\n",
      "\tThere were 30 claims out of 526 sentences (5.70%)\n",
      "Program id 8610238\n",
      "\tThere were 20 claims out of 526 sentences (3.80%)\n",
      "Program id 8567636\n",
      "Annotation bug. A single question mark was annotated..\n",
      "\tThere were 29 claims out of 340 sentences (8.53%)\n",
      "Program id 8\n",
      "\tThere were 32 claims out of 406 sentences (7.88%)\n",
      "Program id 2\n",
      "\tThere were 33 claims out of 320 sentences (10.31%)\n",
      "\n",
      "In total there were 578 claims out of 8466 sentences (6.83%)\n",
      "HELP\n",
      "8567181\n",
      "[[0, 7], [25, 34]]\n",
      "Sukker, [new claim]: Hvedemel,\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 7], [25, 34]]\n",
      "Sukker, [new claim]: Hvedemel,\n",
      "\n",
      "HELP\n",
      "8567181\n",
      "[[0, 22], [23, 68]]\n",
      "Vi får masser af brød. [new claim]: Vi får det bare ikke fra hvede, byg og havre.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 68]]\n",
      "Vi får masser af brød. Vi får det bare ikke fra hvede, byg og havre.\n",
      "\n",
      "HELP\n",
      "5\n",
      "[[0, 71], [72, 112]]\n",
      "Det Europæiske Energiagentur har gjort op at Danmark er et af de lande, [new claim]: der er længst væk fra at opfylde målene.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 112]]\n",
      "Det Europæiske Energiagentur har gjort op at Danmark er et af de lande, der er længst væk fra at opfylde målene.\n",
      "\n",
      "HELP\n",
      "5\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "\n",
      "HELP\n",
      "4\n",
      "[[0, 57], [58, 136]]\n",
      "Mange af problemerne er skabt af den tidligere præsident, [new claim]: altså Bush fordi han har skubbet uro ind i områder, der før var under kontrol.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 136]]\n",
      "Mange af problemerne er skabt af den tidligere præsident, altså Bush fordi han har skubbet uro ind i områder, der før var under kontrol.\n",
      "\n",
      "HELP\n",
      "3\n",
      "[[0, 18], [19, 60]]\n",
      "Jeg kan frygte, at [new claim]: DR1 bliver en slags TV2 i middelmådighed.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 60]]\n",
      "Jeg kan frygte, at DR1 bliver en slags TV2 i middelmådighed.\n",
      "\n",
      "HELP\n",
      "6\n",
      "[[0, 60], [61, 105]]\n",
      "Efter syv år med fremgang er virkeligheden vendt for Venstre [new claim]: som får historiske smæk i meningsmålingerne.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 105]]\n",
      "Efter syv år med fremgang er virkeligheden vendt for Venstre som får historiske smæk i meningsmålingerne.\n",
      "\n",
      "HELP\n",
      "6\n",
      "[[0, 9], [10, 38]]\n",
      "Jeg tror, [new claim]: vi får en stigende ledighed.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 38]]\n",
      "Jeg tror, vi får en stigende ledighed.\n",
      "\n",
      "HELP\n",
      "8635201\n",
      "[[0, 114], [105, 126]]\n",
      "Virkeligheden, når det her loft træder i kraft til oktober, er at en enlig mor med to børn vil få 17.400 udbetalt. [new claim]: udbetalt. Efter skat.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 126]]\n",
      "Virkeligheden, når det her loft træder i kraft til oktober, er at en enlig mor med to børn vil få 17.400 udbetalt. Efter skat.\n",
      "\n",
      "HELP\n",
      "8635201\n",
      "[[115, 144], [145, 160]]\n",
      "30.000 mennesker bliver ramt! [new claim]: Og 35.000 børn.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[115, 160]]\n",
      "30.000 mennesker bliver ramt! Og 35.000 børn.\n",
      "\n",
      "HELP\n",
      "8635201\n",
      "[[0, 37], [38, 161]]\n",
      "Tre ud af fire er ikke arbejdsparate. [new claim]: De kommer ikke pludselig i arbejde fordi vi sænker lønningerne eller frarøver dem boligydelsen der kan forsørge deres børn.\n",
      "Trying to handle this multi-claim, is the output correct?\n",
      "[[0, 161]]\n",
      "Tre ud af fire er ikke arbejdsparate. De kommer ikke pludselig i arbejde fordi vi sænker lønningerne eller frarøver dem boligydelsen der kan forsørge deres børn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, labels = annotatedData.getAllCleanedProgramSentences(disp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Combine timestamps from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['program1', 'program10', 'program8', 'program5', 'program7', 'program4', 'program9', 'program6', 'program3', 'program2'])\n",
      "\n",
      "dict_keys(['end time', 'start time', 'sentences'])\n"
     ]
    }
   ],
   "source": [
    "with Path(save_loc, 'sample_programs.pickle').open('rb') as f:\n",
    "    sample_dict = pickle.load(f)\n",
    "    \n",
    "print(sample_dict.keys())\n",
    "print()\n",
    "print(sample_dict['program1'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with Path('../../data/DeepFactData/preannotated/all_programs.pickle').open('rb') as f:\n",
    "    full_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['start time', 'end time', 'program_id', 'sentence_id', 'sentence', 'claim_idx', 'claim']\n"
     ]
    }
   ],
   "source": [
    "N = len(data) # Observations\n",
    "features = ['start time', 'end time']\n",
    "[features.append(lab) for lab in labels]\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['2222122', '4042060', '4056784', '4914022', '2020168', '4254890', '5559662', '8567636', '2593416', '4879003', '1767521', '4753167', '8415978', '4810657', '2304494', '4121766', '8524981', '4624779', '4717985', '3445438', '7103349', '6500671', '8568658', '4171702', '2271997', '2370770', '2337314', '6528165', '7115504', '3588676', '3550014', '7186318', '8490432', '3693287', '4845589', '2466477', '1748819', '3632763', '1425182', '1777290', '8564875', '4103318', '6066238', '3648942', '2294023', '2455625', '2250789', '5300885', '2261432', '6129385', '6570340', '6486894', '3505973', '2669333', '4223161', '6113249', '2359717', '2523604', '8364372', '1932144', '4071354', '1817762', '6556275', '1739323', '3411204', '2500720', '6443809', '1989107', '5714306', '5171937', '8610238', '2092024', '1425144', '2535266', '2040383', '5085179', '2282900', '5742743', '2061692', '7127382', '6032988', '4266979', '9284846', '2488632', '5694754', '4155377', '2443022', '6542606', '1837743', '2560372', '5252755', '5779637', '3662558', '6153407', '1369970', '6909844', '1787519', '5728721', '2315222', '1347559', '4207467', '4541683', '5639601', '5186364', '7077142', '5984995', '7139464', '7174872', '5964390', '4682044', '8720741', '4859285', '5126491', '3427186', '5675832', '5578714', '2547764', '5935745', '1978162', '1335758', '4003816', '7198080', '2716313', '6584872', '3570949', '2512262', '1292098', '4960526', '6000178', '6642870', '3526461', '1828042', '4828102', '8562295', '1374585', '6628084', '2765236', '5803916', '2983825', '3467995', '1758460', '5216837', '2232356', '4515168', '1808661', '3608907', '2348260', '5158985', '8665813', '1962590', '5236713', '5624194', '8568906', '2000008', '5655525', '8635201', '4087367', '4784978', '7090401', '4027193', '7162934', '1942591', '6097134', '1797683', '5287397', '2325751', '6183773', '1037984', '8376653', '1952770', '5200088', '3488287', '7146484', '8567181', '3720490', '2573324', '6168649', '5762858', '5270879', '5834387', '2011161', '2624937', '6514284', '1012335', '6458522', '7308025', '4574778', '6081732', '6599121', '6018439', '8689224'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the time from the processed data\n",
    "start_times = []\n",
    "end_times = []\n",
    "processed_programs = []\n",
    "\n",
    "sample_previousID = ['1', '2', '3','4','5','6','7','8','9','10']\n",
    "#sample_programID = ['7308025','2294023','2315222','2337314','2359717',\\\n",
    "#                  '2304494','2348260', '3411204', '3570949', '3662558']\n",
    "\n",
    "\n",
    "for i in range(N):  \n",
    "    pro_id = data[i][0]\n",
    "    \n",
    "    # Note sample_dict[pro_id]['start time'] is a list of start times for all sentences\n",
    "    if pro_id not in processed_programs:\n",
    "        \n",
    "        if pro_id in sample_previousID:\n",
    "            [start_times.append(t) for t in sample_dict['program'+pro_id]['start time']]\n",
    "            [end_times.append(t) for t in sample_dict['program'+pro_id]['end time']]\n",
    "            \n",
    "        else:\n",
    "            [start_times.append(t) for t in full_dict[pro_id]['start time']]\n",
    "            [end_times.append(t) for t in full_dict[pro_id]['end time']]\n",
    "        \n",
    "        processed_programs.append(pro_id)\n",
    "# Sanity check        \n",
    "assert(len(start_times) == len(end_times))\n",
    "assert(len(start_times) == N)\n",
    "\n",
    "# Concat data\n",
    "X = np.concatenate((np.asarray([start_times,end_times]).T, np.asarray(data)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8466"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(start_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ret manglende bindestreg i både de observeret sætning og de annoteret sætninger\n",
    "\n",
    "Grundet en fejl i præprocessering, er fx 'Europa-Parlementet' blevet til 'EuropaParlementet\". Det skal rettes.\n",
    "\n",
    "Derudover skal program1, program2, ... rettes til deres programID i stedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with Path(save_loc, 'sample_programs-mbindestreg.pickle').open('rb') as f:\n",
    "    sample_dict_mbind = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getLeadingSpace(s,start_idx):\n",
    "            # Finds first leading space before index \"start_idx\" in s\n",
    "            if start_idx < 0:\n",
    "                return 0\n",
    "            elif s[start_idx] is ' ' :\n",
    "                return start_idx+1\n",
    "            else:\n",
    "                return getLeadingSpace(s,start_idx-1)\n",
    "\n",
    "def getTailingSpace(s,end_idx):\n",
    "    # Finds first trailing space after index \"end_idx\" in s\n",
    "    if end_idx >= len(s):\n",
    "        return len(s)\n",
    "    elif s[end_idx] is ' ' or end_idx == len(s):\n",
    "        return end_idx\n",
    "    else:\n",
    "        return getTailingSpace(s,end_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['program1', 'program10', 'program8', 'program3', 'program7', 'program4', 'program9', 'program6', 'program5', 'program2'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict_mbind.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['program1', 'program10', 'program8', 'program5', 'program7', 'program4', 'program9', 'program6', 'program3', 'program2'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1',\n",
       " '10',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '8567181',\n",
       " '8567636',\n",
       " '8568658',\n",
       " '8568906',\n",
       " '8610238',\n",
       " '8635201',\n",
       " '8665813',\n",
       " '8689224',\n",
       " '8720741',\n",
       " '9',\n",
       " '9284846'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found:\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2udledning. [new claim]: vi kan gøre noget ved der dækker det lidt over halvdelen. [new claim]: De dækker 56% i alt.\n",
      "[[0, 142], [155, 212], [285, 305]]\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2-udledning.\n",
      "\n",
      "vi kan gøre noget ved der dækker det lidt over halvdelen.\n",
      "\n",
      "De dækker 5-6% i alt.\n",
      "\n",
      "\n",
      "Først når vi får udviklet elmotoren, kan vi knække den belastning som det er på bilerne. Den udvikling dækker 15% af den samlede CO2-udledning. Af den del, vi kan gøre noget ved der dækker det lidt over halvdelen. Det er der, nøglen ligger. Jo, Svend. Husholdningerne er også med. Nej. De dækker 5-6% i alt.\n",
      "\n",
      "[[0, 143], [156, 213], [286, 307]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the fake program names to real program ids\n",
    "bugged_programs = ['program1','program2','program3','program4','program5',\\\n",
    "                  'program6','program7', 'program8', 'program9', 'program10']\n",
    "#bugged_programs = ['1', '2', '3','4','5','6','7','8','9','10']\n",
    "\n",
    "actual_programID = ['7308025','2294023','2315222','2337314','2359717',\\\n",
    "                  '2304494','2348260', '3411204', '3570949', '3662558']\n",
    "program_mapping = dict(zip(bugged_programs, actual_programID))\n",
    "\n",
    "## FIX inconsistencies related to the inclusion of \"-\" in the paragraphs\n",
    "for program in bugged_programs: # Fix each of the bugged programs\n",
    "    \n",
    "    idx_X = np.where('program'+X[:,2] == program)[0] #Index in X\n",
    "    \n",
    "    for elem in range(idx_X.shape[0]): # For each paragraph\n",
    "        \n",
    "        X[idx_X[elem],2] = program_mapping[program]\n",
    "        \n",
    "        para_bugged = X[idx_X[elem], 4]\n",
    "        para_true = sample_dict_mbind[program]['sentences'][elem]\n",
    "        # Replace the bugged sentence with the corrected one\n",
    "        X[idx_X[elem], 4] = para_true\n",
    "        \n",
    "        \n",
    "        if X[idx_X[elem],6]: # If there is a claim\n",
    "            #print(X[idx_X[elem],5])\n",
    "            \n",
    "            if len(X[idx_X[elem],5]) == 1:\n",
    "                start_id = X[idx_X[elem],5][0][0]\n",
    "                end_id = X[idx_X[elem],5][0][1]\n",
    "                \n",
    "                claim_idx = [getLeadingSpace(para_true,start_id),\\\n",
    "                             getTailingSpace(para_true,end_id)]\n",
    "                \n",
    "                X[idx_X[elem],5][0] = claim_idx\n",
    "                X[idx_X[elem],6] = para_true[claim_idx[0]:claim_idx[1]]\n",
    "            else:\n",
    "                claim_idx = []\n",
    "                \n",
    "                print('Found:\\n%s' %X[idx_X[elem],6])\n",
    "                print(X[idx_X[elem],5])\n",
    "                \n",
    "                for c in range(len(X[idx_X[elem],5])):\n",
    "                    \n",
    "                    start_id = X[idx_X[elem],5][c][0]\n",
    "                    end_id = X[idx_X[elem],5][c][1]\n",
    "\n",
    "                    claim_idx.append([getLeadingSpace(para_true,start_id),\\\n",
    "                                 getTailingSpace(para_true,end_id)])\n",
    "                    \n",
    "                for idx in claim_idx:\n",
    "                    print(para_true[idx[0]:idx[1]]+'\\n')\n",
    "\n",
    "                X[idx_X[elem],5] = claim_idx\n",
    "                #X[idx_X[elem],6] = ?????\n",
    "                \n",
    "                print()\n",
    "                print(para_true)\n",
    "                \n",
    "                print()\n",
    "                print(claim_idx)\n",
    "                print()\n",
    "            \n",
    "        # Replace the bugged claim with the corrected one\n",
    "        # Correct the claim index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_claims = np.where([elem is not None for elem in X[:,6]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Two claims where actually just mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An errornous annotation\n",
      "['20:09:03:23' '20:09:10:03' '7308025' 87\n",
      " 'Man diskuterer, om DONG skal privatiseres eller styres offentligt.'\n",
      " list([[4, 15]]) 'diskuterer,']\n",
      "The claim is removed\n",
      "['20:09:03:23' '20:09:10:03' '7308025' 87\n",
      " 'Man diskuterer, om DONG skal privatiseres eller styres offentligt.' None\n",
      " None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_claim_idx = np.where(X[:,2]=='7308025')[0][87-1]\n",
    "\n",
    "if X[remove_claim_idx,6]:\n",
    "    print('An errornous annotation')\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print('The claim is removed')\n",
    "    assert('diskuterer,'==X[remove_claim_idx,6])\n",
    "    X[remove_claim_idx,5] = None\n",
    "    X[remove_claim_idx,6] = None\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print()\n",
    "else:\n",
    "    print('No claim to remove\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An errornous annotation\n",
      "['00:13:13:24' '00:13:22:19' '3411204' 120\n",
      " 'For et år siden sagde Fogh, at vi ville komme til at mangle hænder. Jeg sagde, ledigheden ville stige. Fogh er her ikke mere.'\n",
      " list([[79, 89]]) 'ledigheden']\n",
      "The claim is removed\n",
      "['00:13:13:24' '00:13:22:19' '3411204' 120\n",
      " 'For et år siden sagde Fogh, at vi ville komme til at mangle hænder. Jeg sagde, ledigheden ville stige. Fogh er her ikke mere.'\n",
      " None None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remove_claim_idx = np.where(X[:,2]=='3411204')[0][120-1]\n",
    "\n",
    "if X[remove_claim_idx,6]:\n",
    "    print('An errornous annotation')\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print('The claim is removed')\n",
    "    assert('ledigheden'==X[remove_claim_idx,6])\n",
    "    X[remove_claim_idx,5] = None\n",
    "    X[remove_claim_idx,6] = None\n",
    "    print(X[remove_claim_idx,:])\n",
    "    print()\n",
    "else:\n",
    "    print('No claim to remove\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''for i in all_claims:\n",
    "    print('Program %s, \\t sentence %i'%(X[i,2], X[i,3]))\n",
    "    print(X[i,4])\n",
    "    print(X[i,5])\n",
    "    print(X[i,6])\n",
    "    print()\n",
    "    ''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An errornous annotation\n",
      "['00:05:59:18' '00:06:03:02' '8567636' 62\n",
      " 'Bente Klarlund, du forsker i fedme. Er det de fedes egen skyld?'\n",
      " list([[6, 15]]) 'Klarlund,']\n",
      "Klarlund,\n",
      "The claim is removed\n",
      "['00:05:59:18' '00:06:03:02' '8567636' 62\n",
      " 'Bente Klarlund, du forsker i fedme. Er det de fedes egen skyld?' None\n",
      " None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['13:43:28:20' '13:43:34:07' '8665813' 123\n",
      " 'Vil vi opdage, at dem, der ikke får 4-taller, er fra ikke-boglige hjem?'\n",
      " list([[0, 3]]) 'Vil']\n",
      "Vil\n",
      "The claim is removed\n",
      "['13:43:28:20' '13:43:34:07' '8665813' 123\n",
      " 'Vil vi opdage, at dem, der ikke får 4-taller, er fra ikke-boglige hjem?'\n",
      " None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['13:31:04:15' '13:31:17:13' '8665813' 2\n",
      " 'Men nu vil regeringen indføre karakterkrav til gymnasiet så nogle vil måske blive valgt fra. Hvorfor er det i Danmarks interesse at have færre unge med en studentereksamen?'\n",
      " list([[93, 100]]) 'Hvorfor']\n",
      "Hvorfor\n",
      "The claim is removed\n",
      "['13:31:04:15' '13:31:17:13' '8665813' 2\n",
      " 'Men nu vil regeringen indføre karakterkrav til gymnasiet så nogle vil måske blive valgt fra. Hvorfor er det i Danmarks interesse at have færre unge med en studentereksamen?'\n",
      " None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['20:01:24:00' '20:01:29:01' '9284846' 14\n",
      " 'Paw Karslund er folketingskandidat for DF.' list([[39, 42]]) 'DF.']\n",
      "DF.\n",
      "The claim is removed\n",
      "['20:01:24:00' '20:01:29:01' '9284846' 14\n",
      " 'Paw Karslund er folketingskandidat for DF.' None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['00:01:23:02' '00:01:31:08' '8567181' 12\n",
      " 'Men jeg har altid skrantet. Det samme gjorde min søn som fik diagnosen infantil autist. Han var enormt adfærdsvanskelig.'\n",
      " list([[4, 11]]) 'jeg har']\n",
      "jeg har\n",
      "The claim is removed\n",
      "['00:01:23:02' '00:01:31:08' '8567181' 12\n",
      " 'Men jeg har altid skrantet. Det samme gjorde min søn som fik diagnosen infantil autist. Han var enormt adfærdsvanskelig.'\n",
      " None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['00:00:07:12' '00:00:16:20' '8567181' 2\n",
      " 'I stedet vitamintilskud og valleprotein, soja, kokos og naturmedicin. Holistiske udrensninger og indre oliebade.'\n",
      " list([[81, 93]]) 'udrensninger']\n",
      "udrensninger\n",
      "The claim is removed\n",
      "['00:00:07:12' '00:00:16:20' '8567181' 2\n",
      " 'I stedet vitamintilskud og valleprotein, soja, kokos og naturmedicin. Holistiske udrensninger og indre oliebade.'\n",
      " None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['00:00:03:17' '00:00:07:09' '8567181' 1\n",
      " 'Sukker, nej. Mælk, niks. Hvedemel, nul.' list([[0, 7], [25, 34]])\n",
      " 'Sukker, [new claim]: Hvedemel,']\n",
      "Sukker, [new claim]: Hvedemel,\n",
      "The claim is removed\n",
      "['00:00:03:17' '00:00:07:09' '8567181' 1\n",
      " 'Sukker, nej. Mælk, niks. Hvedemel, nul.' None None]\n",
      "\n",
      "---------------------------------------------------------\n",
      "An errornous annotation\n",
      "['20:20:25:07' '20:20:35:02' '7308025' 185\n",
      " 'Vi accepterer en struktur, som er designet ... Jeg kan ikke forstå det. Kan du høre, det lyder mærkeligt? Messerschmidt.'\n",
      " list([[0, 26]]) 'Vi accepterer en struktur,']\n",
      "Vi accepterer en struktur,\n",
      "The claim is removed\n",
      "['20:20:25:07' '20:20:35:02' '7308025' 185\n",
      " 'Vi accepterer en struktur, som er designet ... Jeg kan ikke forstå det. Kan du høre, det lyder mærkeligt? Messerschmidt.'\n",
      " None None]\n",
      "\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def remove_claims(X, r_program, r_sentence, r_claim):\n",
    "    \n",
    "    \n",
    "    for i in range(len(r_program)):\n",
    "        #print(r_program[i])\n",
    "        #print(np.where(X[:,2]==r_program[i]))\n",
    "        remove_claim_idx = np.where(X[:,2]==r_program[i])[0][r_sentence[i]-1]\n",
    "\n",
    "        if X[remove_claim_idx,6]:\n",
    "            print('An errornous annotation')\n",
    "            print(X[remove_claim_idx,:])\n",
    "            print(r_claim[i])\n",
    "            assert(r_claim[i]==X[remove_claim_idx,6])\n",
    "            X[remove_claim_idx,5] = None\n",
    "            X[remove_claim_idx,6] = None\n",
    "            print('The claim is removed')\n",
    "            print(X[remove_claim_idx,:])\n",
    "            print()\n",
    "        else:\n",
    "            print('No claim to remove\\n')\n",
    "        \n",
    "        print('---------------------------------------------------------')\n",
    "    return X\n",
    "\n",
    "programs_id_of_the_claim = ['8567636','8665813','8665813','9284846', '8567181', '8567181',\\\n",
    "                            '8567181', '7308025']\n",
    "sentence_id_of_the_claim = [62,123,2,14, 12, 2, \\\n",
    "                            1, 185]\n",
    "the_claim_to_be_removed = ['Klarlund,',\\\n",
    "                           'Vil',\\\n",
    "                           'Hvorfor',\\\n",
    "                           'DF.',\\\n",
    "                           'jeg har',\\\n",
    "                           'udrensninger',\\\n",
    "                           'Sukker, [new claim]: Hvedemel,',\\\n",
    "                           'Vi accepterer en struktur,']\n",
    "\n",
    "X = remove_claims(X, programs_id_of_the_claim, sentence_id_of_the_claim,\\\n",
    "                  the_claim_to_be_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of claims: 568\n",
      "Total number of paragraphs: 8466\n",
      "Procentage claims is 6.71%\n"
     ]
    }
   ],
   "source": [
    "description = \"\"\"Claim detection in the television program DR Debatten\n",
    "\n",
    "In several Debatten programs interesting/relevant claims were annotated by DR.\n",
    "All sentences (or paragraphs) of these programs were extracted and marked as containing a\n",
    "claim or not. The claims themselves are also extracted.\n",
    "\n",
    "Currently the data contains N={:d} sentences/paragraphs from {:d} programs. The data is represented\n",
    "as a matrix of size N x M, where M is a number of attributes for each sentence.\n",
    "\n",
    "These attributes are:\n",
    "    'start time'      The start time of a sentence/paragraph (h:m:s:ms)\n",
    "    'end time'        The end time of a sentence/paragraph\n",
    "    'program_id'      Indicates which Debatten program is the origin of the sentence\n",
    "    'sentences_id'    Indicates which sentence in the program it is\n",
    "                      (ordered from 1 to 2 to .. to the last sentence)\n",
    "    'sentence'        A string with the full sentence/paragraph\n",
    "    'claim_idx'       [start, end]-index of the claim (in the sentence)\n",
    "    'claim'           A string with the claim\n",
    "\n",
    "The data is available as a .csv file and .pickle file (python3).\n",
    "\n",
    "Copyright(R): This data is made available in connection with the course \"02456 Deep Learning\" \n",
    "at the Technical University of Denmark, during the Fall 2017. Redistribution or commercial use\n",
    "of the dataset is not allowed without prior agreement.\n",
    "\n",
    "\n",
    "-------- Python3 Example: Load data, transform to Bag-of-Words and fit a logistic regression ----\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "with open(\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "X = data['data'][:,4]\n",
    "y = data['data'][:,6]\n",
    "\n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(len(X))])       \n",
    "\n",
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "confusion_matrix(y, ypred)\n",
    "\n",
    "\"\"\".format(N,len(processed_programs))\n",
    "\n",
    "\n",
    "print('Total number of claims: %i'%(len(np.where([elem is not None for elem in X[:,6]])[0])))\n",
    "print('Total number of paragraphs: %i'%(len(X[:,6])))\n",
    "print('Procentage claims is {:2.2f}%'.format(len(np.where([elem is not None for elem in X[:,6]])[0])/float(len(X[:,6]))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with Path(save_loc, 'readme_sample_programs.txt').open('w') as f:\n",
    "    f.write(description)\n",
    "\n",
    "# Add features as top row\n",
    "Y = np.concatenate((np.asarray(features).reshape(1,-1), X), axis=0);\n",
    "\n",
    "with Path(save_loc, \"data_matrix_sample_programs.csv\").open(\"w\", newline=\"\\n\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file, delimiter=\",\")\n",
    "    for row in Y:\n",
    "        # row = [str(val).encode(\"utf-8\") for val in row]\n",
    "        # print(row[4])\n",
    "        # row[4] = row[4].encode(\"utf-8\")\n",
    "        # print(\"   \", row[4])\n",
    "        # print(\"   \", row[4].decode())\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "with Path(save_loc, \"data_matrix_sample_programs.pickle\").open('wb') as f:\n",
    "        pickle.dump(dict(zip(['data','features'],[X, features])), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
