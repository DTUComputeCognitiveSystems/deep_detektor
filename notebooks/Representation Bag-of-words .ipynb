{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Example: Load data, transform to Bag-of-words and fit Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import stop_words\n",
    "\n",
    "load_loc = '/home/jehi/Dropbox/DTU/DeepFactData/annotated/' #SPECIFY!\n",
    "with open(load_loc+\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start time',\n",
       " 'end time',\n",
       " 'program_id',\n",
       " 'sentence_id',\n",
       " 'sentence',\n",
       " 'claim_idx',\n",
       " 'claim']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data['features']\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The sentences are for the data matrix (X), while the claims is for the outcome vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is:\n",
      "\tDet er også kedeligt, at man taler med én stemme om klimaet.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tMen hvad er ved at ske?\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tVi lever ikke i Europas Forenede Stater endnu så hvorfor skal vi have en præsident?\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tVi har en formand for Kommissionen og for Parlamentet.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tDet kan jeg nu ikke huske. Det er lang tid siden men det her er en formand for statsministerklubben, og det er godt.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tLad os tage et konkret eksempel.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tDe har diskuteret finansiering af den mulige klimaaftale i et helt år uden at kunne blive enige.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data['data'][:,4]\n",
    "y = data['data'][:,6]\n",
    "N = len(X)\n",
    "\n",
    "# Some examples\n",
    "for i in [21, 23, 33, 40, 48, 49, 50]: #range(100):\n",
    "    print('Sentence is:\\n\\t' + X[i])\n",
    "    print('Claim is:\\n\\t' + str(y[i])+'\\n')\n",
    "       \n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transform and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessing_bow(s):\n",
    "    # replace special character with word (so it doesnt get lost)\n",
    "    s = s.replace('%',' procent ')\n",
    "\n",
    "    # Split numbers from words\n",
    "    s = ' '.join(re.split('([\\d,.-]+)',s))\n",
    "\n",
    "    # Split all -\n",
    "    s = ' '.join(s.split('-'))\n",
    "    return s\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocessing_bow(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3225,    0],\n",
       "       [ 116,  148]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the confusion matrix as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.92433362,  0.        ],\n",
       "       [ 0.03324735,  0.04241903]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C/np.sum(C[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most words only have one occurence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 6162\n",
      "Words with 1 occurence: 3405\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a list of danish stop words\n",
    "stop_list = stop_words.get_stop_words('danish')\n",
    "stop_list.remove('meget')\n",
    "stop_list.remove('mange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3225,    0],\n",
       "       [ 136,  128]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer(stop_words=stop_list)\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glem alt om kommunalvalg ,  rævekager ,  de lange knives nat og lokalt fnidder . \n",
      "['Glem', 'alt', 'om', 'kommunalvalg', ',', 'rævekager', ',', 'de', 'lange', 'knives', 'nat', 'og', 'lokalt', 'fnidder', '.']\n",
      "['glem', 'alt', 'om', 'kommunalvalg', ',', 'rævekag', ',', 'de', 'lang', 'kniv', 'nat', 'og', 'lokalt', 'fnid', '.']\n"
     ]
    }
   ],
   "source": [
    "s = X[0]\n",
    "print(s)\n",
    "print(word_tokenize(s, language='danish'))\n",
    "#print(sent_tokenize(s, language='danish'))\n",
    "stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "print([stemmer.stem(elm) for elm in word_tokenize(s, language='danish')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stemming each paragraph, then joining the paragraph into a dataset\n",
    "stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "X_stem = []\n",
    "for paragraf in X:\n",
    "    X_stem.append(' '.join([stemmer.stem(el) for el in word_tokenize(paragraf, language='danish')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glem alt om kommunalvalg , rævekag , de lang kniv nat og lokalt fnid .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stemmed_bow(X, vectorizer):\n",
    "    stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "    \n",
    "    X_stem = []\n",
    "    for paragraf in X:\n",
    "        X_stem.append(' '.join([stemmer.stem(el) for el in word_tokenize(paragraf, language='danish')]))\n",
    "    \n",
    "    return vectorizer.fit_transform(X_stem)\n",
    "    \n",
    "#vectorizer = CountVectorizer(stop_words=stop_list)\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3222,    3],\n",
       "       [ 135,  129]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = stemmed_bow(X, vectorizer)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 4295\n",
      "Words with 1 occurence: 2082\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_idx = (a>1).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2213"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = X_bow[:,keep_idx]\n",
    "\n",
    "new_words = []\n",
    "for i in range(len(words)):\n",
    "    if keep_idx[i]:\n",
    "        new_words.append(words[i])\n",
    "        \n",
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3220,    5],\n",
       "       [ 155,  109]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 2213\n",
      "Words with 1 occurence: 0\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(new_words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Not easily done for the danish language.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "'dan' in wn.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('hund',lang='dan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
