{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Example: Load data, transform to Bag-of-words and fit Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import stop_words\n",
    "\n",
    "load_loc = '../../data/DeepFactData/annotated/' #SPECIFY!\n",
    "with open(load_loc+\"data_matrix_sample_programs.pickle\",'rb') as f:\n",
    "        data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['start time',\n",
       " 'end time',\n",
       " 'program_id',\n",
       " 'sentence_id',\n",
       " 'sentence',\n",
       " 'claim_idx',\n",
       " 'claim']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data['features']\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The sentences are for the data matrix (X), while the claims is for the outcome vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence is:\n",
      "\tLA og DF, der sammen med K udgør det parlamentariske grundlag rasede mod partiet, mens Løkke opfordrede til en tænkepause.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tDe har sat personfnidder over fornuftig borgerlig politik.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tAt vi skal bruge så meget energi på at nå dertil hvor hun ville indrømme, at der måske manglede en lille stjerne men ellers så var alt retvisende, og alt var lagt frem.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tIngen kan være i tvivl om, at kronen på Eva Kjers politiske værk det er den her landbrugspakke, som er hendes prestigeprojekt.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tVi kan sagtens være åbne og ærlige og fremlægge det for befolkningen og overbevise dem om, at det er det rigtige.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tDet er ikke kun mig, der mener, tallene ikke har været forklaret.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n",
      "Sentence is:\n",
      "\tDu sagde noget forkert. Du sagde, at det kun var mig, der mente det.\n",
      "Claim is:\n",
      "\tNone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = data['data'][:,4]\n",
    "y = data['data'][:,6]\n",
    "N = len(X)\n",
    "\n",
    "# Some examples\n",
    "for i in [21, 23, 33, 40, 48, 49, 50]: #range(100):\n",
    "    print('Sentence is:\\n\\t' + X[i])\n",
    "    print('Claim is:\\n\\t' + str(y[i])+'\\n')\n",
    "       \n",
    "# Now convert y to a binary indicator matrix (1 is claim, 0 no claim)\n",
    "y = np.asarray([y[i] is not None for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Transform and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessing_bow(s):\n",
    "    # replace special character with word (so it doesnt get lost)\n",
    "    s = s.replace('%',' procent ')\n",
    "\n",
    "    # Split numbers from words\n",
    "    s = ' '.join(re.split('([\\d,.-]+)',s))\n",
    "\n",
    "    # Split all -\n",
    "    s = ' '.join(s.split('-'))\n",
    "    return s\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X[i] = preprocessing_bow(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7893,    5],\n",
       "       [ 292,  276]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get the confusion matrix as probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.32317505e-01,   5.90597685e-04],\n",
       "       [  3.44909048e-02,   3.26009922e-02]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C/np.sum(C[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Most words only have one occurence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 10383\n",
      "Words with 1 occurence: 5363\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Try removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a list of danish stop words\n",
    "stop_list = stop_words.get_stop_words('danish')\n",
    "stop_list.remove('meget')\n",
    "stop_list.remove('mange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7894,    4],\n",
       "       [ 326,  242]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a Bag-of-Words\n",
    "vectorizer = CountVectorizer(stop_words=stop_list)\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tokenize the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nu er det godt og vel  24  timer siden ,  at Lars Løkke bekendtgjorde at han måtte undersøge ,  om der er et grundlag for hans regering . \n",
      "['Nu', 'er', 'det', 'godt', 'og', 'vel', '24', 'timer', 'siden', ',', 'at', 'Lars', 'Løkke', 'bekendtgjorde', 'at', 'han', 'måtte', 'undersøge', ',', 'om', 'der', 'er', 'et', 'grundlag', 'for', 'hans', 'regering', '.']\n",
      "['nu', 'er', 'det', 'godt', 'og', 'vel', '24', 'tim', 'sid', ',', 'at', 'lar', 'løk', 'bekendtgjord', 'at', 'han', 'måt', 'undersøg', ',', 'om', 'der', 'er', 'et', 'grundlag', 'for', 'han', 'regering', '.']\n"
     ]
    }
   ],
   "source": [
    "s = X[0]\n",
    "print(s)\n",
    "print(word_tokenize(s, language='danish'))\n",
    "#print(sent_tokenize(s, language='danish'))\n",
    "stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "print([stemmer.stem(elm) for elm in word_tokenize(s, language='danish')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Stemming each paragraph, then joining the paragraph into a dataset\n",
    "stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "X_stem = []\n",
    "for paragraf in X:\n",
    "    X_stem.append(' '.join([stemmer.stem(el) for el in word_tokenize(paragraf, language='danish')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nu er det godt og vel 24 tim sid , at lar løk bekendtgjord at han måt undersøg , om der er et grundlag for han regering .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_stem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def stemmed_bow(X, vectorizer):\n",
    "    stemmer = nltk.stem.SnowballStemmer('danish')\n",
    "    \n",
    "    X_stem = []\n",
    "    for paragraf in X:\n",
    "        X_stem.append(' '.join([stemmer.stem(el) for el in word_tokenize(paragraf, language='danish')]))\n",
    "    \n",
    "    return vectorizer.fit_transform(X_stem)\n",
    "    \n",
    "#vectorizer = CountVectorizer(stop_words=stop_list)\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7885,   13],\n",
       "       [ 336,  232]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = stemmed_bow(X, vectorizer)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 7038\n",
      "Words with 1 occurence: 3297\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "keep_idx = (a>1).reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3741"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bow = X_bow[:,keep_idx]\n",
    "\n",
    "new_words = []\n",
    "for i in range(len(words)):\n",
    "    if keep_idx[i]:\n",
    "        new_words.append(words[i])\n",
    "        \n",
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7883,   15],\n",
       "       [ 370,  198]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the logit model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X=X_bow,y=y)\n",
    "ypred = logistic.predict(X_bow) \n",
    "\n",
    "# CM on traning data\n",
    "C = confusion_matrix(y, ypred)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 3741\n",
      "Words with 1 occurence: 0\n"
     ]
    }
   ],
   "source": [
    "a=np.asarray(np.sum(X_bow,axis=0))\n",
    "\n",
    "print('Total words %i' %len(new_words))\n",
    "print('Words with 1 occurence: %i' %np.sum(a==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Lemmatization\n",
    "Not easily done for the danish language.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "'dan' in wn.langs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('hund',lang='dan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
